{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "h7MSEcenjVrL"
   },
   "source": [
    "notebook1\n",
    "## PART 1. Document retrieval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 清理舊測試資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf data/hanlp_con_test_results.pkl\n",
    "# !rm -rf data/test_doc5.jsonl\n",
    "!rm -rf data/test_doc5sent5.jsonl\n",
    "!rm -rf data/test_doc5sent5.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_model = 'step2_new_upload'\n",
    "step3_model = 'step3_new_upload'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the environment and import all library we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "niqu9pLajYC_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# built-in libs\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "\n",
    "# 3rd party libs\n",
    "import hanlp\n",
    "import opencc\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "from hanlp.components.pipeline import Pipeline\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "# our own libs\n",
    "\n",
    "\n",
    "def load_json(file_path: Union[Path, str]) -> pd.DataFrame:\n",
    "    \"\"\"jsonl_to_df read jsonl file and return a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path (Union[Path, str]): The jsonl file path.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The jsonl file content.\n",
    "\n",
    "    Example:\n",
    "        >>> read_jsonl_file(\"data/train.jsonl\")\n",
    "               id            label  ... predicted_label                                      evidence_list\n",
    "        0    3984          refutes  ...         REFUTES  [城市規劃是城市建設及管理的依據 ， 位於城市管理之規劃 、 建設 、 運作三個階段之首 ，...\n",
    "        ..    ...              ...  ...             ...                                                ...\n",
    "        945  3042         supports  ...         REFUTES  [北歐人相傳每當雷雨交加時就是索爾乘坐馬車出來巡視 ， 因此稱呼索爾為 “ 雷神 ” 。, ...\n",
    "\n",
    "        [946 rows x 10 columns]\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf8\") as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    return [json.loads(json_str) for json_str in json_list]\n",
    "\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)\n",
    "wikipedia.set_lang(\"zh\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_DATA = load_json(\"data/public_test.jsonl\")\n",
    "CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
    "CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data class for type hinting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Claim:\n",
    "    data: str\n",
    "\n",
    "@dataclass\n",
    "class AnnotationID:\n",
    "    id: int\n",
    "\n",
    "@dataclass\n",
    "class EvidenceID:\n",
    "    id: int\n",
    "\n",
    "@dataclass\n",
    "class PageTitle:\n",
    "    title: str\n",
    "\n",
    "@dataclass\n",
    "class SentenceID:\n",
    "    id: int\n",
    "\n",
    "@dataclass\n",
    "class Evidence:\n",
    "    data: List[List[Tuple[AnnotationID, EvidenceID, PageTitle, SentenceID]]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of consistency, we convert traditional to simplified Chinese first before converting it back to traditional Chinese.  This is due to some errors occuring when converting traditional to traditional Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "A3NU01DnjKp-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_st_corrections(text: str) -> str:\n",
    "    simplified = CONVERTER_T2S.convert(text)\n",
    "\n",
    "    return CONVERTER_S2T.convert(simplified)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use constituency parsing to separate part of speeches or so called constituent to extract noun phrases.  In the later stages, we will use the noun phrases as the query to search for relevant documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nps_hanlp(\n",
    "    predictor: Pipeline,\n",
    "    d: Dict[str, Union[int, Claim, Evidence]],\n",
    ") -> List[str]:\n",
    "    claim = d[\"claim\"]\n",
    "    tree = predictor(claim)[\"con\"]\n",
    "    nps = [\n",
    "        do_st_corrections(\"\".join(subtree.leaves()))\n",
    "        for subtree in tree.subtrees(lambda t: t.label() == \"NP\")\n",
    "    ]\n",
    "\n",
    "    return nps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision refers to how many related documents are retrieved.  Recall refers to how many relevant documents are retrieved.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_precision(\n",
    "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "    predictions: pd.Series,\n",
    ") -> None:\n",
    "    precision = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        # Extract all ground truth of titles of the wikipedia pages\n",
    "        # evidence[2] refers to the title of the wikipedia page\n",
    "        gt_pages = set([\n",
    "            evidence[2]\n",
    "            for evidence_set in d[\"evidence\"]\n",
    "            for evidence in evidence_set\n",
    "        ])\n",
    "\n",
    "        predicted_pages = predictions.iloc[i]\n",
    "        hits = predicted_pages.intersection(gt_pages)\n",
    "        if len(predicted_pages) != 0:\n",
    "            precision += len(hits) / len(predicted_pages)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # Macro precision\n",
    "    print(f\"Precision: {precision / count}\")\n",
    "\n",
    "\n",
    "def calculate_recall(\n",
    "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "    predictions: pd.Series,\n",
    ") -> None:\n",
    "    recall = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        gt_pages = set([\n",
    "            evidence[2]\n",
    "            for evidence_set in d[\"evidence\"]\n",
    "            for evidence in evidence_set\n",
    "        ])\n",
    "        predicted_pages = predictions.iloc[i]\n",
    "        hits = predicted_pages.intersection(gt_pages)\n",
    "        recall += len(hits) / len(gt_pages)\n",
    "        count += 1\n",
    "\n",
    "    print(f\"Recall: {recall / count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default amount of documents retrieved is at most five documents.  This `num_pred_doc` can be adjusted based on your objective.  Save data in jsonl format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_doc(\n",
    "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "    predictions: pd.Series,\n",
    "    mode: str = \"train\",\n",
    "    num_pred_doc: int = 5,\n",
    ") -> None:\n",
    "    with open(\n",
    "        f\"data/{mode}_doc{num_pred_doc}.jsonl\",\n",
    "        \"w\",\n",
    "        encoding=\"utf8\",\n",
    "    ) as f:\n",
    "        for i, d in enumerate(data):\n",
    "            d[\"predicted_pages\"] = list(predictions.iloc[i])\n",
    "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function for document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ayGI44qkk_wy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pred_pages(series_data: pd.Series) -> Set[Dict[int, str]]:\n",
    "    results = []\n",
    "    tmp_muji = []\n",
    "    # wiki_page: its index showned in claim\n",
    "    mapping = {}\n",
    "    claim = series_data[\"claim\"]\n",
    "    nps = series_data[\"hanlp_results\"]\n",
    "    first_wiki_term = []\n",
    "\n",
    "    for i, np in enumerate(nps):\n",
    "        # Simplified Traditional Chinese Correction\n",
    "        wiki_search_results = [\n",
    "            do_st_corrections(w) for w in wikipedia.search(np)\n",
    "        ]\n",
    "\n",
    "        # Remove the wiki page's description in brackets\n",
    "        wiki_set = [re.sub(r\"\\s\\(\\S+\\)\", \"\", w) for w in wiki_search_results]\n",
    "        wiki_df = pd.DataFrame({\n",
    "            \"wiki_set\": wiki_set,\n",
    "            \"wiki_results\": wiki_search_results\n",
    "        })\n",
    "\n",
    "        # Elements in wiki_set --> index\n",
    "        # Extracting only the first element is one way to avoid extracting\n",
    "        # too many of the similar wiki pages\n",
    "        grouped_df = wiki_df.groupby(\"wiki_set\", sort=False).first()\n",
    "        candidates = grouped_df[\"wiki_results\"].tolist()\n",
    "        # muji refers to wiki_set\n",
    "        muji = grouped_df.index.tolist()\n",
    "\n",
    "        for prefix, term in zip(muji, candidates):\n",
    "            if prefix not in tmp_muji:\n",
    "                matched = False\n",
    "\n",
    "                # Take at least one term from the first noun phrase\n",
    "                if i == 0:\n",
    "                    first_wiki_term.append(term)\n",
    "\n",
    "                # Walrus operator :=\n",
    "                # https://docs.python.org/3/whatsnew/3.8.html#assignment-expressions\n",
    "                # Through these filters, we are trying to figure out if the term\n",
    "                # is within the claim\n",
    "                if (((new_term := term) in claim) or\n",
    "                    ((new_term := term.replace(\"·\", \"\")) in claim) or\n",
    "                    ((new_term := term.split(\" \")[0]) in claim) or\n",
    "                    ((new_term := term.replace(\"-\", \" \")) in claim)):\n",
    "                    matched = True\n",
    "\n",
    "                elif \"·\" in term:\n",
    "                    splitted = term.split(\"·\")\n",
    "                    for split in splitted:\n",
    "                        if (new_term := split) in claim:\n",
    "                            matched = True\n",
    "                            break\n",
    "\n",
    "                if matched:\n",
    "                    # post-processing\n",
    "                    term = term.replace(\" \", \"_\")\n",
    "                    term = term.replace(\"-\", \"\")\n",
    "                    results.append(term)\n",
    "                    mapping[term] = claim.find(new_term)\n",
    "                    tmp_muji.append(new_term)\n",
    "\n",
    "    # 5 is a hyperparameter\n",
    "    if len(results) > 5:\n",
    "        assert -1 not in mapping.values()\n",
    "        results = sorted(mapping, key=mapping.get)\n",
    "    elif len(results) < 1:\n",
    "        results = first_wiki_term\n",
    "\n",
    "    return set(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Get noun phrases from hanlp consituency parsing tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup [HanLP](https://github.com/hankcs/HanLP) predictor (1 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    }
   ],
   "source": [
    "predictor = (hanlp.pipeline().append(\n",
    "    hanlp.load(\"MSR_TOK_ELECTRA_BASE_CRF\"),\n",
    "    output_key=\"tok\",\n",
    ").append(\n",
    "    hanlp.load(\"CTB9_CON_ELECTRA_SMALL\"),\n",
    "    output_key=\"con\",\n",
    "    input_key=\"tok\",\n",
    "))\n",
    "# FINE_ELECTRA_SMALL_ZH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will skip this process which for creating parsing tree when demo on class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Repeat the same process on test set\n",
    "Create parsing tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hanlp_test_file = f\"data/hanlp_con_test_results.pkl\"\n",
    "if Path(hanlp_test_file).exists():\n",
    "    with open(hanlp_test_file, \"rb\") as f:\n",
    "        hanlp_results = pickle.load(f)\n",
    "else:\n",
    "    hanlp_results = [get_nps_hanlp(predictor, d) for d in TEST_DATA]\n",
    "    with open(hanlp_test_file, \"wb\") as f:\n",
    "        pickle.dump(hanlp_results, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get pages via wiki online api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86900b6be094b37803412c1843d4d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=904), Label(value='0 / 904'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_doc_path = f\"data/test_doc5.jsonl\"\n",
    "if Path(test_doc_path).exists():\n",
    "    with open(test_doc_path, \"r\", encoding=\"utf8\") as f:\n",
    "        test_results = pd.Series(\n",
    "            [set(json.loads(line)[\"predicted_pages\"]) for line in f])\n",
    "else:\n",
    "    test_df = pd.DataFrame(TEST_DATA)\n",
    "    test_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
    "    test_results = test_df.parallel_apply(get_pred_pages, axis=1)\n",
    "    save_doc(TEST_DATA, test_results, mode=\"test\")\n",
    "print(test_results.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ol4zFkSbjgXF"
   },
   "source": [
    "notebook2\n",
    "## PART 2. Sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlliDsgXjisj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# built-in libs\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "\n",
    "# third-party libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from transformers import (\n",
    "#     AutoModelForSequenceClassification,\n",
    "#     AutoTokenizer,\n",
    "#     get_scheduler,\n",
    "# )\n",
    "# local libs\n",
    "from typing import Dict, Union\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "# import torch\n",
    "# from transformers import get_scheduler\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "\n",
    "def load_json(file_path: Union[Path, str]) -> pd.DataFrame:\n",
    "    \"\"\"jsonl_to_df read jsonl file and return a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path (Union[Path, str]): The jsonl file path.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The jsonl file content.\n",
    "\n",
    "    Example:\n",
    "        >>> read_jsonl_file(\"data/train.jsonl\")\n",
    "               id            label  ... predicted_label                                      evidence_list\n",
    "        0    3984          refutes  ...         REFUTES  [城市規劃是城市建設及管理的依據 ， 位於城市管理之規劃 、 建設 、 運作三個階段之首 ，...\n",
    "        ..    ...              ...  ...             ...                                                ...\n",
    "        945  3042         supports  ...         REFUTES  [北歐人相傳每當雷雨交加時就是索爾乘坐馬車出來巡視 ， 因此稱呼索爾為 “ 雷神 ” 。, ...\n",
    "\n",
    "        [946 rows x 10 columns]\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf8\") as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    return [json.loads(json_str) for json_str in json_list]\n",
    "\n",
    "\n",
    "def jsonl_dir_to_df(dir_path: Union[Path, str]) -> pd.DataFrame:\n",
    "    \"\"\"jsonl_dir_to_df read jsonl dir and return a pandas DataFrame.\n",
    "\n",
    "    This function will read all jsonl files in the dir_path and concat them.\n",
    "\n",
    "    Args:\n",
    "        dir_path (Union[Path, str]): The jsonl dir path.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The jsonl dir content.\n",
    "\n",
    "    Example:\n",
    "        >>> read_jsonl_dir(\"data/extracted_dir/\")\n",
    "               id            label  ... predicted_label                                      evidence_list\n",
    "        0    3984          refutes  ...         REFUTES  [城市規劃是城市建設及管理的依據 ， 位於城市管理之規劃 、 建設 、 運作三個階段之首 ，...\n",
    "        ..    ...              ...  ...             ...                                                ...\n",
    "        945  3042         supports  ...         REFUTES  [北歐人相傳每當雷雨交加時就是索爾乘坐馬車出來巡視 ， 因此稱呼索爾為 “ 雷神 ” 。, ...\n",
    "\n",
    "        [946 rows x 10 columns]\n",
    "    \"\"\"\n",
    "    print(f\"Reading and concatenating jsonl files in {dir_path}\")\n",
    "    return pd.concat(\n",
    "        [pd.DataFrame(load_json(file)) for file in Path(dir_path).glob(\"*.jsonl\")]\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_evidence_to_wiki_pages_mapping(\n",
    "    wiki_pages: pd.DataFrame,\n",
    ") -> Dict[str, Dict[int, str]]:\n",
    "    \"\"\"generate_wiki_pages_dict generate a mapping from evidence to wiki pages by evidence id.\n",
    "\n",
    "    Args:\n",
    "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
    "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
    "            If cache is None, return the result directly.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    def make_dict(x):\n",
    "        result = {}\n",
    "        sentences = re.split(r\"\\n(?=[0-9])\", x)\n",
    "        for sent in sentences:\n",
    "            splitted = sent.split(\"\\t\")\n",
    "            if len(splitted) < 2:\n",
    "                # Avoid empty articles\n",
    "                return result\n",
    "            result[splitted[0]] = splitted[1]\n",
    "        return result\n",
    "\n",
    "    # copy wiki_pages\n",
    "    wiki_pages = wiki_pages.copy()\n",
    "\n",
    "    # generate parse mapping\n",
    "    print(\"Generate parse mapping\")\n",
    "    wiki_pages[\"evidence_map\"] = wiki_pages[\"lines\"].parallel_map(make_dict)\n",
    "    # generate id to evidence_map mapping\n",
    "    print(\"Transform to id to evidence_map mapping\")\n",
    "    mapping = dict(\n",
    "        zip(\n",
    "            wiki_pages[\"id\"].to_list(),\n",
    "            wiki_pages[\"evidence_map\"].to_list(),\n",
    "        )\n",
    "    )\n",
    "    # release memory\n",
    "    del wiki_pages\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def save_checkpoint(model, ckpt_dir: str, current_step: int, mark: str = \"\"):\n",
    "    if mark != \"\":\n",
    "        mark += \"_\"\n",
    "    torch.save(model.state_dict(), f\"{ckpt_dir}/{mark}model.{current_step}.pt\")\n",
    "\n",
    "\n",
    "def load_model(model, ckpt_name, ckpt_dir: str):\n",
    "    model.load_state_dict(torch.load(f\"{ckpt_dir}/{ckpt_name}\"))\n",
    "    return model\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3BBLE3_hlPi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "TEST_DATA = load_json(\"data/public_test.jsonl\")\n",
    "\n",
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "_y = [LABEL2ID[data[\"label\"]] for data in TRAIN_DATA]\n",
    "# GT means Ground Truth\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload wiki database (1 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)\n",
    "del wiki_pages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision for sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evidence_macro_precision(\n",
    "    instance: Dict,\n",
    "    top_rows: pd.DataFrame,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculate precision for sentence retrieval\n",
    "    This function is modified from fever-scorer.\n",
    "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
    "\n",
    "    Args:\n",
    "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
    "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
    "\n",
    "        IMPORTANT!!!\n",
    "        instance (dict) should have the key of `evidence`.\n",
    "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]:\n",
    "        [1]: relevant and retrieved (numerator of precision)\n",
    "        [2]: retrieved (denominator of precision)\n",
    "    \"\"\"\n",
    "    this_precision = 0.0\n",
    "    this_precision_hits = 0.0\n",
    "\n",
    "    # Return 0, 0 if label is not enough info since not enough info does not\n",
    "    # contain any evidence.\n",
    "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
    "        # e[2] is the page title, e[3] is the sentence index\n",
    "        all_evi = [[e[2], e[3]]\n",
    "                   for eg in instance[\"evidence\"]\n",
    "                   for e in eg\n",
    "                   if e[3] is not None]\n",
    "        claim = instance[\"claim\"]\n",
    "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
    "                                      claim][\"predicted_evidence\"].tolist()\n",
    "\n",
    "        for prediction in predicted_evidence:\n",
    "            if prediction in all_evi:\n",
    "                this_precision += 1.0\n",
    "            this_precision_hits += 1.0\n",
    "\n",
    "        return (this_precision /\n",
    "                this_precision_hits) if this_precision_hits > 0 else 1.0, 1.0\n",
    "\n",
    "    return 0.0, 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate recall for sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evidence_macro_recall(\n",
    "    instance: Dict,\n",
    "    top_rows: pd.DataFrame,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculate recall for sentence retrieval\n",
    "    This function is modified from fever-scorer.\n",
    "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
    "\n",
    "    Args:\n",
    "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
    "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
    "\n",
    "        IMPORTANT!!!\n",
    "        instance (dict) should have the key of `evidence`.\n",
    "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]:\n",
    "        [1]: relevant and retrieved (numerator of recall)\n",
    "        [2]: relevant (denominator of recall)\n",
    "    \"\"\"\n",
    "    # We only want to score F1/Precision/Recall of recalled evidence for NEI claims\n",
    "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
    "        # If there's no evidence to predict, return 1\n",
    "        if len(instance[\"evidence\"]) == 0 or all(\n",
    "            [len(eg) == 0 for eg in instance]):\n",
    "            return 1.0, 1.0\n",
    "\n",
    "        claim = instance[\"claim\"]\n",
    "\n",
    "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
    "                                      claim][\"predicted_evidence\"].tolist()\n",
    "\n",
    "        for evidence_group in instance[\"evidence\"]:\n",
    "            evidence = [[e[2], e[3]] for e in evidence_group]\n",
    "            if all([item in predicted_evidence for item in evidence]):\n",
    "                # We only want to score complete groups of evidence. Incomplete\n",
    "                # groups are worthless.\n",
    "                return 1.0, 1.0\n",
    "        return 0.0, 1.0\n",
    "    return 0.0, 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the scores of sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_retrieval(\n",
    "    probs: np.ndarray,\n",
    "    df_evidences: pd.DataFrame,\n",
    "    ground_truths: pd.DataFrame,\n",
    "    top_n: int = 5,\n",
    "    cal_scores: bool = True,\n",
    "    save_name: str = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Calculate the scores of sentence retrieval\n",
    "\n",
    "    Args:\n",
    "        probs (np.ndarray): probabilities of the candidate retrieved sentences\n",
    "        df_evidences (pd.DataFrame): the candiate evidence sentences paired with claims\n",
    "        ground_truths (pd.DataFrame): the loaded data of dev.jsonl or test.jsonl\n",
    "        top_n (int, optional): the number of the retrieved sentences. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: F1 score, precision, and recall\n",
    "    \"\"\"\n",
    "    df_evidences[\"prob\"] = probs\n",
    "    top_rows = (\n",
    "        df_evidences.groupby(\"claim\").apply(\n",
    "        lambda x: x.nlargest(top_n, \"prob\"))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if cal_scores:\n",
    "        macro_precision = 0\n",
    "        macro_precision_hits = 0\n",
    "        macro_recall = 0\n",
    "        macro_recall_hits = 0\n",
    "\n",
    "        for i, instance in enumerate(ground_truths):\n",
    "            macro_prec = evidence_macro_precision(instance, top_rows)\n",
    "            macro_precision += macro_prec[0]\n",
    "            macro_precision_hits += macro_prec[1]\n",
    "\n",
    "            macro_rec = evidence_macro_recall(instance, top_rows)\n",
    "            macro_recall += macro_rec[0]\n",
    "            macro_recall_hits += macro_rec[1]\n",
    "\n",
    "        pr = (macro_precision /\n",
    "              macro_precision_hits) if macro_precision_hits > 0 else 1.0\n",
    "        rec = (macro_recall /\n",
    "               macro_recall_hits) if macro_recall_hits > 0 else 0.0\n",
    "        f1 = 2.0 * pr * rec / (pr + rec)\n",
    "\n",
    "    if save_name is not None:\n",
    "        # write doc7_sent5 file\n",
    "        with open(f\"data/{save_name}\", \"w\") as f:\n",
    "            for instance in ground_truths:\n",
    "                claim = instance[\"claim\"]\n",
    "                predicted_evidence = top_rows[\n",
    "                    top_rows[\"claim\"] == claim][\"predicted_evidence\"].tolist()\n",
    "                instance[\"predicted_evidence\"] = predicted_evidence\n",
    "                f.write(json.dumps(instance, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    if cal_scores:\n",
    "        return {\"F1 score\": f1, \"Precision\": pr, \"Recall\": rec}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference script to get probabilites for the candidate evidence sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function for sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpvXpFwXszfv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pair_with_wiki_sentences(\n",
    "    mapping: Dict[str, Dict[int, str]],\n",
    "    df: pd.DataFrame,\n",
    "    negative_ratio: float,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Only for creating train sentences.\"\"\"\n",
    "    claims = []\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    idx = []\n",
    "    # positive\n",
    "    mappinglabel = {'supports':1, 'refutes':2}\n",
    "    for i in range(len(df)):\n",
    "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "        evidence_sets = df[\"evidence\"].iloc[i]\n",
    "        labelmap = mappinglabel[ df[\"label\"].iloc[i] ]\n",
    "        for evidence_set in evidence_sets:\n",
    "            sents = []\n",
    "            for evidence in evidence_set:\n",
    "                # evidence[2] is the page title\n",
    "                page = evidence[2].replace(\" \", \"_\")\n",
    "                # the only page with weird name\n",
    "                if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
    "                    continue\n",
    "                # evidence[3] is in form of int however, mapping requires str\n",
    "                sent_idx = str(evidence[3])\n",
    "                sents.append(mapping[page][sent_idx])\n",
    "\n",
    "            whole_evidence = \" \".join(sents)\n",
    "\n",
    "            claims.append(claim)\n",
    "            sentences.append(whole_evidence.replace(\" \",\"\"))\n",
    "            # idx.append(evidence[2])\n",
    "            labels.append(1)\n",
    "\n",
    "    # negative\n",
    "    for i in range(len(df)):\n",
    "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "\n",
    "        evidence_set = set([(evidence[2], evidence[3])\n",
    "                            for evidences in df[\"evidence\"][i]\n",
    "                            for evidence in evidences])\n",
    "        predicted_pages = df[\"predicted_pages\"][i]\n",
    "        for page in predicted_pages:\n",
    "            page = page.replace(\" \", \"_\")\n",
    "            try:\n",
    "                page_sent_id_pairs = [\n",
    "                    (page, sent_idx) for sent_idx in mapping[page].keys()\n",
    "                ]\n",
    "            except KeyError:\n",
    "                # print(f\"{page} is not in our Wiki db.\")\n",
    "                continue\n",
    "\n",
    "            for pair in page_sent_id_pairs:\n",
    "                if pair in evidence_set:\n",
    "                    continue\n",
    "                text = mapping[page][pair[1]]\n",
    "                # `np.random.rand(1) <= 0.05`: Control not to add too many negative samples\n",
    "                if text != \"\" and np.random.rand(1) <= negative_ratio:\n",
    "                    claims.append(claim)\n",
    "                    sentences.append(text.replace(\" \",\"\"))\n",
    "                    labels.append(0)\n",
    "                    # idx.append(page)\n",
    "\n",
    "    # return pd.DataFrame({\"claim\": claims, \"text\": sentences, \"idx\": idx, \"label\": labels})\n",
    "    return pd.DataFrame({\"claim\": claims, \"text\": sentences,  \"label\": labels})\n",
    "\n",
    "\n",
    "def pair_with_wiki_sentences_eval(\n",
    "    mapping: Dict[str, Dict[int, str]],\n",
    "    df: pd.DataFrame,\n",
    "    is_testset: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Only for creating dev and test sentences.\"\"\"\n",
    "    claims = []\n",
    "    sentences = []\n",
    "    evidence = []\n",
    "    idx = []\n",
    "    predicted_evidence = []\n",
    "\n",
    "    # negative\n",
    "    for i in range(len(df)):\n",
    "        # if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "        #     continue\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "\n",
    "        predicted_pages = df[\"predicted_pages\"][i]\n",
    "        for page in predicted_pages:\n",
    "            page = page.replace(\" \", \"_\")\n",
    "            try:\n",
    "                page_sent_id_pairs = [(page, k) for k in mapping[page]]\n",
    "            except KeyError:\n",
    "                # print(f\"{page} is not in our Wiki db.\")\n",
    "                continue\n",
    "\n",
    "            for page_name, sentence_id in page_sent_id_pairs:\n",
    "                text = mapping[page][sentence_id]\n",
    "                if text != \"\":\n",
    "                    claims.append(claim)\n",
    "                    sentences.append(text.replace(\" \",\"\"))\n",
    "                    idx.append(page)\n",
    "                    if not is_testset:\n",
    "                        evidence.append(df[\"evidence\"].iloc[i])\n",
    "                    predicted_evidence.append([page_name, int(sentence_id)])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"claim\": claims,\n",
    "        \"text\": sentences,\n",
    "        # \"idx\": idx,\n",
    "        \"evidence\": evidence if not is_testset else None,\n",
    "        \"predicted_evidence\": predicted_evidence,\n",
    "    })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Setup training environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title  { display-mode: \"form\" }\n",
    "TOP_N = 5  #@param {type:\"integer\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation part (15 mins)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Check on our test data\n",
    "(5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "predictor = MultiModalPredictor.load(step2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVFusJqjmex-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = load_json(\"data/test_doc5.jsonl\")\n",
    "\n",
    "test_evidences = pair_with_wiki_sentences_eval(\n",
    "    mapping,\n",
    "    pd.DataFrame(test_data),\n",
    "    is_testset=True,\n",
    ")\n",
    "test_evidences.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Start predicting the test data\")\n",
    "probs = predictor.predict_proba(test_evidences)\n",
    "probs = probs.to_numpy()\n",
    "second_values = [p[1] for p in probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_retrieval(\n",
    "    probs=second_values,\n",
    "    df_evidences=test_evidences,\n",
    "    ground_truths=test_data,\n",
    "    top_n=TOP_N,\n",
    "    cal_scores=False,\n",
    "    save_name=f\"test_doc5sent{TOP_N}.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_evidences.head(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lGzl8a5JteT7",
    "tags": []
   },
   "source": [
    "notebook3\n",
    "## PART 3. Claim verification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgA1vcUyzjlx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    ")\n",
    "from utils import (\n",
    "    generate_evidence_to_wiki_pages_mapping,\n",
    "    jsonl_dir_to_df,\n",
    "    load_json,\n",
    "    load_model,\n",
    "    save_checkpoint,\n",
    ")\n",
    "\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload wiki database (same as part 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages,)\n",
    "del wiki_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"wiki_pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def join_with_topk_evidence(\n",
    "    df: pd.DataFrame,\n",
    "    mapping: dict,\n",
    "    mode: str = \"train\",\n",
    "    topk: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"join_with_topk_evidence join the dataset with topk evidence.\n",
    "\n",
    "    Note:\n",
    "        After extraction, the dataset will be like this:\n",
    "               id     label         claim                           evidence            evidence_list\n",
    "        0    4604  supports       高行健...     [[[3393, 3552, 高行健, 0], [...  [高行健 （ ）江西赣州出...\n",
    "        ..    ...       ...            ...                                ...                     ...\n",
    "        945  2095  supports       美國總...  [[[1879, 2032, 吉米·卡特, 16], [...  [卸任后 ， 卡特積極參與...\n",
    "        停各种战争及人質危機的斡旋工作 ， 反对美国小布什政府攻打伊拉克...\n",
    "\n",
    "        [946 rows x 5 columns]\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset with evidence.\n",
    "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
    "        topk (int, optional): The topk evidence. Defaults to 5.\n",
    "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
    "            If cache is None, return the result directly.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with topk evidence_list.\n",
    "            The `evidence_list` column will be: List[str]\n",
    "    \"\"\"\n",
    "\n",
    "    # format evidence column to List[List[Tuple[str, str, str, str]]]\n",
    "    if \"evidence\" in df.columns:\n",
    "        df[\"evidence\"] = df[\"evidence\"].parallel_map(\n",
    "            lambda x: [[x]] if not isinstance(x[0], list) else [x]\n",
    "            if not isinstance(x[0][0], list) else x)\n",
    "\n",
    "    print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
    "    df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(lambda x: [\n",
    "            mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "            for evi_id, evi_idx in x  # for each evidence list\n",
    "        ][:topk] if isinstance(x, list) else [])\n",
    "    print(df[\"evidence_list\"][:5])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Setup training environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title  { display-mode: \"form\" }\n",
    "\n",
    "EVIDENCE_TOPK = 5  #@param {type:\"integer\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Concat claim and evidences\n",
    "join topk evidence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prevent CUDA out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training (30 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_to_string(lst):\n",
    "    # 將 list 轉換為 string，並刪除所有空格\n",
    "    return \"[PAD]\".join(str(item).replace(' ', '') for item in lst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Make your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLkfuoAE49mz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "predictor = MultiModalPredictor.load(step3_model)\n",
    "TEST_DATA = load_json(\"data/test_doc5sent5.jsonl\")\n",
    "TEST_PKL_FILE = Path(\"data/test_doc5sent5.pkl\")\n",
    "\n",
    "test_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(TEST_DATA),\n",
    "        mapping,\n",
    "        mode=\"eval\",\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "test_df.to_pickle(TEST_PKL_FILE, protocol=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df_org = test_df.copy()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = test_df[[\"claim\",\"evidence_list\"]]\n",
    "test_df['evidence_list'] = test_df['evidence_list'].apply(list_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_label = predictor.predict(test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_dataset = test_df_org\n",
    "predict_dataset[\"predicted_label\"] = predicted_label\n",
    "OUTPUT_FILENAME = \"submission.jsonl\"\n",
    "predict_dataset[[\"id\", \"predicted_label\", \"predicted_evidence\"]].to_json(\n",
    "    OUTPUT_FILENAME,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將預測 SUPPORTS 或是 REFUTES 但 predicted_evidence 是空的更改成 NOT ENOUGH INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_json\n",
    "data = load_json(OUTPUT_FILENAME)\n",
    "# data predicted_evidence is 0 change this label\n",
    "for item in data:\n",
    "    if item['predicted_label'] == 'NOT ENOUGH INFO':\n",
    "        continue\n",
    "    length = len(item['predicted_evidence'])\n",
    "    if length == 0:\n",
    "        print(item)\n",
    "        item['predicted_label'] = 'NOT ENOUGH INFO'\n",
    "# save this json \n",
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_json(OUTPUT_FILENAME+\"_Del\", orient='records', lines=True, force_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "10286f3c74912972f7d1fdceceee5be5b7c77248e5efe5afcbc6a71f24d230fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

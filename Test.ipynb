{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7MSEcenjVrL"
   },
   "source": [
    "notebook1\n",
    "## PART 1. Document retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 清理舊測試資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf data/hanlp_con_test_results.pkl\n",
    "# !rm -rf data/test_doc5.jsonl\n",
    "!rm -rf data/test_doc5sent5.jsonl\n",
    "!rm -rf data/test_doc5sent5.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_model = 'step2_new_upload'\n",
    "step3_model = 'step3_base_upload'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the environment and import all library we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "niqu9pLajYC_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# built-in libs\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "\n",
    "# 3rd party libs\n",
    "import hanlp\n",
    "import opencc\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "from hanlp.components.pipeline import Pipeline\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "# our own libs\n",
    "\n",
    "\n",
    "def load_json(file_path: Union[Path, str]) -> pd.DataFrame:\n",
    "    \"\"\"jsonl_to_df read jsonl file and return a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path (Union[Path, str]): The jsonl file path.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The jsonl file content.\n",
    "\n",
    "    Example:\n",
    "        >>> read_jsonl_file(\"data/train.jsonl\")\n",
    "               id            label  ... predicted_label                                      evidence_list\n",
    "        0    3984          refutes  ...         REFUTES  [城市規劃是城市建設及管理的依據 ， 位於城市管理之規劃 、 建設 、 運作三個階段之首 ，...\n",
    "        ..    ...              ...  ...             ...                                                ...\n",
    "        945  3042         supports  ...         REFUTES  [北歐人相傳每當雷雨交加時就是索爾乘坐馬車出來巡視 ， 因此稱呼索爾為 “ 雷神 ” 。, ...\n",
    "\n",
    "        [946 rows x 10 columns]\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf8\") as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    return [json.loads(json_str) for json_str in json_list]\n",
    "\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)\n",
    "wikipedia.set_lang(\"zh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_DATA = load_json(\"data/public_test.jsonl\")\n",
    "CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
    "CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data class for type hinting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Claim:\n",
    "    data: str\n",
    "\n",
    "@dataclass\n",
    "class AnnotationID:\n",
    "    id: int\n",
    "\n",
    "@dataclass\n",
    "class EvidenceID:\n",
    "    id: int\n",
    "\n",
    "@dataclass\n",
    "class PageTitle:\n",
    "    title: str\n",
    "\n",
    "@dataclass\n",
    "class SentenceID:\n",
    "    id: int\n",
    "\n",
    "@dataclass\n",
    "class Evidence:\n",
    "    data: List[List[Tuple[AnnotationID, EvidenceID, PageTitle, SentenceID]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of consistency, we convert traditional to simplified Chinese first before converting it back to traditional Chinese.  This is due to some errors occuring when converting traditional to traditional Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "A3NU01DnjKp-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_st_corrections(text: str) -> str:\n",
    "    simplified = CONVERTER_T2S.convert(text)\n",
    "\n",
    "    return CONVERTER_S2T.convert(simplified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use constituency parsing to separate part of speeches or so called constituent to extract noun phrases.  In the later stages, we will use the noun phrases as the query to search for relevant documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nps_hanlp(\n",
    "    predictor: Pipeline,\n",
    "    d: Dict[str, Union[int, Claim, Evidence]],\n",
    ") -> List[str]:\n",
    "    claim = d[\"claim\"]\n",
    "    tree = predictor(claim)[\"con\"]\n",
    "    nps = [\n",
    "        do_st_corrections(\"\".join(subtree.leaves()))\n",
    "        for subtree in tree.subtrees(lambda t: t.label() == \"NP\")\n",
    "    ]\n",
    "\n",
    "    return nps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision refers to how many related documents are retrieved.  Recall refers to how many relevant documents are retrieved.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default amount of documents retrieved is at most five documents.  This `num_pred_doc` can be adjusted based on your objective.  Save data in jsonl format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_doc(\n",
    "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "    predictions: pd.Series,\n",
    "    mode: str = \"train\",\n",
    "    num_pred_doc: int = 5,\n",
    ") -> None:\n",
    "    with open(\n",
    "        f\"data/{mode}_doc{num_pred_doc}.jsonl\",\n",
    "        \"w\",\n",
    "        encoding=\"utf8\",\n",
    "    ) as f:\n",
    "        for i, d in enumerate(data):\n",
    "            d[\"predicted_pages\"] = list(predictions.iloc[i])\n",
    "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function for document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ayGI44qkk_wy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pred_pages(series_data: pd.Series) -> Set[Dict[int, str]]:\n",
    "    results = []\n",
    "    tmp_muji = []\n",
    "    # wiki_page: its index showned in claim\n",
    "    mapping = {}\n",
    "    claim = series_data[\"claim\"]\n",
    "    nps = series_data[\"hanlp_results\"]\n",
    "    first_wiki_term = []\n",
    "\n",
    "    for i, np in enumerate(nps):\n",
    "        # Simplified Traditional Chinese Correction\n",
    "        wiki_search_results = [\n",
    "            do_st_corrections(w) for w in wikipedia.search(np)\n",
    "        ]\n",
    "\n",
    "        # Remove the wiki page's description in brackets\n",
    "        wiki_set = [re.sub(r\"\\s\\(\\S+\\)\", \"\", w) for w in wiki_search_results]\n",
    "        wiki_df = pd.DataFrame({\n",
    "            \"wiki_set\": wiki_set,\n",
    "            \"wiki_results\": wiki_search_results\n",
    "        })\n",
    "\n",
    "        # Elements in wiki_set --> index\n",
    "        # Extracting only the first element is one way to avoid extracting\n",
    "        # too many of the similar wiki pages\n",
    "        grouped_df = wiki_df.groupby(\"wiki_set\", sort=False).first()\n",
    "        candidates = grouped_df[\"wiki_results\"].tolist()\n",
    "        # muji refers to wiki_set\n",
    "        muji = grouped_df.index.tolist()\n",
    "\n",
    "        for prefix, term in zip(muji, candidates):\n",
    "            if prefix not in tmp_muji:\n",
    "                matched = False\n",
    "\n",
    "                # Take at least one term from the first noun phrase\n",
    "                if i == 0:\n",
    "                    first_wiki_term.append(term)\n",
    "\n",
    "                # Walrus operator :=\n",
    "                # https://docs.python.org/3/whatsnew/3.8.html#assignment-expressions\n",
    "                # Through these filters, we are trying to figure out if the term\n",
    "                # is within the claim\n",
    "                if (((new_term := term) in claim) or\n",
    "                    ((new_term := term.replace(\"·\", \"\")) in claim) or\n",
    "                    ((new_term := term.split(\" \")[0]) in claim) or\n",
    "                    ((new_term := term.replace(\"-\", \" \")) in claim)):\n",
    "                    matched = True\n",
    "\n",
    "                elif \"·\" in term:\n",
    "                    splitted = term.split(\"·\")\n",
    "                    for split in splitted:\n",
    "                        if (new_term := split) in claim:\n",
    "                            matched = True\n",
    "                            break\n",
    "\n",
    "                if matched:\n",
    "                    # post-processing\n",
    "                    term = term.replace(\" \", \"_\")\n",
    "                    term = term.replace(\"-\", \"\")\n",
    "                    results.append(term)\n",
    "                    mapping[term] = claim.find(new_term)\n",
    "                    tmp_muji.append(new_term)\n",
    "\n",
    "    # 5 is a hyperparameter\n",
    "    if len(results) > 5:\n",
    "        assert -1 not in mapping.values()\n",
    "        results = sorted(mapping, key=mapping.get)\n",
    "    elif len(results) < 1:\n",
    "        results = first_wiki_term\n",
    "\n",
    "    return set(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Get noun phrases from hanlp consituency parsing tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup [HanLP](https://github.com/hankcs/HanLP) predictor (1 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    }
   ],
   "source": [
    "predictor = (hanlp.pipeline().append(\n",
    "    hanlp.load(\"MSR_TOK_ELECTRA_BASE_CRF\"),\n",
    "    output_key=\"tok\",\n",
    ").append(\n",
    "    hanlp.load(\"CTB9_CON_ELECTRA_SMALL\"),\n",
    "    output_key=\"con\",\n",
    "    input_key=\"tok\",\n",
    "))\n",
    "# FINE_ELECTRA_SMALL_ZH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will skip this process which for creating parsing tree when demo on class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Repeat the same process on test set\n",
    "Create parsing tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hanlp_test_file = f\"data/hanlp_con_test_results.pkl\"\n",
    "if Path(hanlp_test_file).exists():\n",
    "    with open(hanlp_test_file, \"rb\") as f:\n",
    "        hanlp_results = pickle.load(f)\n",
    "else:\n",
    "    hanlp_results = [get_nps_hanlp(predictor, d) for d in TEST_DATA]\n",
    "    with open(hanlp_test_file, \"wb\") as f:\n",
    "        pickle.dump(hanlp_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get pages via wiki online api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        {電磁學, 物, 光學顯微鏡, 肉眼, 顯微鏡}\n",
      "1                         {蠶, 昆蟲}\n",
      "2        {波蘭, 人, 西部, 0, 土地, 綠山城縣}\n",
      "3               {魂斷藍橋, 配角, 藍橋, 橋}\n",
      "4    {文學, 劇情片, 侯孝賢, 金馬獎, 文言文, 電影}\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "test_doc_path = f\"data/test_doc5.jsonl\"\n",
    "if Path(test_doc_path).exists():\n",
    "    with open(test_doc_path, \"r\", encoding=\"utf8\") as f:\n",
    "        test_results = pd.Series(\n",
    "            [set(json.loads(line)[\"predicted_pages\"]) for line in f])\n",
    "else:\n",
    "    test_df = pd.DataFrame(TEST_DATA)\n",
    "    test_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
    "    test_results = test_df.parallel_apply(get_pred_pages, axis=1)\n",
    "    save_doc(TEST_DATA, test_results, mode=\"test\")\n",
    "print(test_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ol4zFkSbjgXF"
   },
   "source": [
    "notebook2\n",
    "## PART 2. Sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GlliDsgXjisj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# built-in libs\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "\n",
    "# third-party libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from transformers import (\n",
    "#     AutoModelForSequenceClassification,\n",
    "#     AutoTokenizer,\n",
    "#     get_scheduler,\n",
    "# )\n",
    "# local libs\n",
    "from typing import Dict, Union\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "# import torch\n",
    "# from transformers import get_scheduler\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "\n",
    "def load_json(file_path: Union[Path, str]) -> pd.DataFrame:\n",
    "    \"\"\"jsonl_to_df read jsonl file and return a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path (Union[Path, str]): The jsonl file path.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The jsonl file content.\n",
    "\n",
    "    Example:\n",
    "        >>> read_jsonl_file(\"data/train.jsonl\")\n",
    "               id            label  ... predicted_label                                      evidence_list\n",
    "        0    3984          refutes  ...         REFUTES  [城市規劃是城市建設及管理的依據 ， 位於城市管理之規劃 、 建設 、 運作三個階段之首 ，...\n",
    "        ..    ...              ...  ...             ...                                                ...\n",
    "        945  3042         supports  ...         REFUTES  [北歐人相傳每當雷雨交加時就是索爾乘坐馬車出來巡視 ， 因此稱呼索爾為 “ 雷神 ” 。, ...\n",
    "\n",
    "        [946 rows x 10 columns]\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf8\") as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    return [json.loads(json_str) for json_str in json_list]\n",
    "\n",
    "\n",
    "def jsonl_dir_to_df(dir_path: Union[Path, str]) -> pd.DataFrame:\n",
    "    \"\"\"jsonl_dir_to_df read jsonl dir and return a pandas DataFrame.\n",
    "\n",
    "    This function will read all jsonl files in the dir_path and concat them.\n",
    "\n",
    "    Args:\n",
    "        dir_path (Union[Path, str]): The jsonl dir path.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The jsonl dir content.\n",
    "\n",
    "    Example:\n",
    "        >>> read_jsonl_dir(\"data/extracted_dir/\")\n",
    "               id            label  ... predicted_label                                      evidence_list\n",
    "        0    3984          refutes  ...         REFUTES  [城市規劃是城市建設及管理的依據 ， 位於城市管理之規劃 、 建設 、 運作三個階段之首 ，...\n",
    "        ..    ...              ...  ...             ...                                                ...\n",
    "        945  3042         supports  ...         REFUTES  [北歐人相傳每當雷雨交加時就是索爾乘坐馬車出來巡視 ， 因此稱呼索爾為 “ 雷神 ” 。, ...\n",
    "\n",
    "        [946 rows x 10 columns]\n",
    "    \"\"\"\n",
    "    print(f\"Reading and concatenating jsonl files in {dir_path}\")\n",
    "    return pd.concat(\n",
    "        [pd.DataFrame(load_json(file)) for file in Path(dir_path).glob(\"*.jsonl\")]\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_evidence_to_wiki_pages_mapping(\n",
    "    wiki_pages: pd.DataFrame,\n",
    ") -> Dict[str, Dict[int, str]]:\n",
    "    \"\"\"generate_wiki_pages_dict generate a mapping from evidence to wiki pages by evidence id.\n",
    "\n",
    "    Args:\n",
    "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
    "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
    "            If cache is None, return the result directly.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    def make_dict(x):\n",
    "        result = {}\n",
    "        sentences = re.split(r\"\\n(?=[0-9])\", x)\n",
    "        for sent in sentences:\n",
    "            splitted = sent.split(\"\\t\")\n",
    "            if len(splitted) < 2:\n",
    "                # Avoid empty articles\n",
    "                return result\n",
    "            result[splitted[0]] = splitted[1]\n",
    "        return result\n",
    "\n",
    "    # copy wiki_pages\n",
    "    wiki_pages = wiki_pages.copy()\n",
    "\n",
    "    # generate parse mapping\n",
    "    print(\"Generate parse mapping\")\n",
    "    wiki_pages[\"evidence_map\"] = wiki_pages[\"lines\"].parallel_map(make_dict)\n",
    "    # generate id to evidence_map mapping\n",
    "    print(\"Transform to id to evidence_map mapping\")\n",
    "    mapping = dict(\n",
    "        zip(\n",
    "            wiki_pages[\"id\"].to_list(),\n",
    "            wiki_pages[\"evidence_map\"].to_list(),\n",
    "        )\n",
    "    )\n",
    "    # release memory\n",
    "    del wiki_pages\n",
    "    return mapping\n",
    "\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "J3BBLE3_hlPi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "TEST_DATA = load_json(\"data/public_test.jsonl\")\n",
    "\n",
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
    "# GT means Ground Truth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload wiki database (1 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and concatenating jsonl files in data/wiki-pages\n",
      "Generate parse mapping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009b67b160964ea180e8a7569cc6af39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=118776), Label(value='0 / 118776')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform to id to evidence_map mapping\n"
     ]
    }
   ],
   "source": [
    "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)\n",
    "del wiki_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision for sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evidence_macro_precision(\n",
    "    instance: Dict,\n",
    "    top_rows: pd.DataFrame,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculate precision for sentence retrieval\n",
    "    This function is modified from fever-scorer.\n",
    "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
    "\n",
    "    Args:\n",
    "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
    "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
    "\n",
    "        IMPORTANT!!!\n",
    "        instance (dict) should have the key of `evidence`.\n",
    "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]:\n",
    "        [1]: relevant and retrieved (numerator of precision)\n",
    "        [2]: retrieved (denominator of precision)\n",
    "    \"\"\"\n",
    "    this_precision = 0.0\n",
    "    this_precision_hits = 0.0\n",
    "\n",
    "    # Return 0, 0 if label is not enough info since not enough info does not\n",
    "    # contain any evidence.\n",
    "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
    "        # e[2] is the page title, e[3] is the sentence index\n",
    "        all_evi = [[e[2], e[3]]\n",
    "                   for eg in instance[\"evidence\"]\n",
    "                   for e in eg\n",
    "                   if e[3] is not None]\n",
    "        claim = instance[\"claim\"]\n",
    "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
    "                                      claim][\"predicted_evidence\"].tolist()\n",
    "\n",
    "        for prediction in predicted_evidence:\n",
    "            if prediction in all_evi:\n",
    "                this_precision += 1.0\n",
    "            this_precision_hits += 1.0\n",
    "\n",
    "        return (this_precision /\n",
    "                this_precision_hits) if this_precision_hits > 0 else 1.0, 1.0\n",
    "\n",
    "    return 0.0, 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate recall for sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evidence_macro_recall(\n",
    "    instance: Dict,\n",
    "    top_rows: pd.DataFrame,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculate recall for sentence retrieval\n",
    "    This function is modified from fever-scorer.\n",
    "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
    "\n",
    "    Args:\n",
    "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
    "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
    "\n",
    "        IMPORTANT!!!\n",
    "        instance (dict) should have the key of `evidence`.\n",
    "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]:\n",
    "        [1]: relevant and retrieved (numerator of recall)\n",
    "        [2]: relevant (denominator of recall)\n",
    "    \"\"\"\n",
    "    # We only want to score F1/Precision/Recall of recalled evidence for NEI claims\n",
    "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
    "        # If there's no evidence to predict, return 1\n",
    "        if len(instance[\"evidence\"]) == 0 or all(\n",
    "            [len(eg) == 0 for eg in instance]):\n",
    "            return 1.0, 1.0\n",
    "\n",
    "        claim = instance[\"claim\"]\n",
    "\n",
    "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
    "                                      claim][\"predicted_evidence\"].tolist()\n",
    "\n",
    "        for evidence_group in instance[\"evidence\"]:\n",
    "            evidence = [[e[2], e[3]] for e in evidence_group]\n",
    "            if all([item in predicted_evidence for item in evidence]):\n",
    "                # We only want to score complete groups of evidence. Incomplete\n",
    "                # groups are worthless.\n",
    "                return 1.0, 1.0\n",
    "        return 0.0, 1.0\n",
    "    return 0.0, 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the scores of sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_retrieval(\n",
    "    probs: np.ndarray,\n",
    "    df_evidences: pd.DataFrame,\n",
    "    ground_truths: pd.DataFrame,\n",
    "    top_n: int = 5,\n",
    "    cal_scores: bool = True,\n",
    "    save_name: str = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Calculate the scores of sentence retrieval\n",
    "\n",
    "    Args:\n",
    "        probs (np.ndarray): probabilities of the candidate retrieved sentences\n",
    "        df_evidences (pd.DataFrame): the candiate evidence sentences paired with claims\n",
    "        ground_truths (pd.DataFrame): the loaded data of dev.jsonl or test.jsonl\n",
    "        top_n (int, optional): the number of the retrieved sentences. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: F1 score, precision, and recall\n",
    "    \"\"\"\n",
    "    df_evidences[\"prob\"] = probs\n",
    "    top_rows = (\n",
    "        df_evidences.groupby(\"claim\").apply(\n",
    "        lambda x: x.nlargest(top_n, \"prob\"))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if cal_scores:\n",
    "        macro_precision = 0\n",
    "        macro_precision_hits = 0\n",
    "        macro_recall = 0\n",
    "        macro_recall_hits = 0\n",
    "\n",
    "        for i, instance in enumerate(ground_truths):\n",
    "            macro_prec = evidence_macro_precision(instance, top_rows)\n",
    "            macro_precision += macro_prec[0]\n",
    "            macro_precision_hits += macro_prec[1]\n",
    "\n",
    "            macro_rec = evidence_macro_recall(instance, top_rows)\n",
    "            macro_recall += macro_rec[0]\n",
    "            macro_recall_hits += macro_rec[1]\n",
    "\n",
    "        pr = (macro_precision /\n",
    "              macro_precision_hits) if macro_precision_hits > 0 else 1.0\n",
    "        rec = (macro_recall /\n",
    "               macro_recall_hits) if macro_recall_hits > 0 else 0.0\n",
    "        f1 = 2.0 * pr * rec / (pr + rec)\n",
    "\n",
    "    if save_name is not None:\n",
    "        # write doc7_sent5 file\n",
    "        with open(f\"data/{save_name}\", \"w\") as f:\n",
    "            for instance in ground_truths:\n",
    "                claim = instance[\"claim\"]\n",
    "                predicted_evidence = top_rows[\n",
    "                    top_rows[\"claim\"] == claim][\"predicted_evidence\"].tolist()\n",
    "                instance[\"predicted_evidence\"] = predicted_evidence\n",
    "                f.write(json.dumps(instance, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    if cal_scores:\n",
    "        return {\"F1 score\": f1, \"Precision\": pr, \"Recall\": rec}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference script to get probabilites for the candidate evidence sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function for sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gpvXpFwXszfv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pair_with_wiki_sentences_eval(\n",
    "    mapping: Dict[str, Dict[int, str]],\n",
    "    df: pd.DataFrame,\n",
    "    is_testset: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Only for creating dev and test sentences.\"\"\"\n",
    "    claims = []\n",
    "    sentences = []\n",
    "    evidence = []\n",
    "    idx = []\n",
    "    predicted_evidence = []\n",
    "\n",
    "    # negative\n",
    "    for i in range(len(df)):\n",
    "        # if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "        #     continue\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "\n",
    "        predicted_pages = df[\"predicted_pages\"][i]\n",
    "        for page in predicted_pages:\n",
    "            page = page.replace(\" \", \"_\")\n",
    "            try:\n",
    "                page_sent_id_pairs = [(page, k) for k in mapping[page]]\n",
    "            except KeyError:\n",
    "                # print(f\"{page} is not in our Wiki db.\")\n",
    "                continue\n",
    "\n",
    "            for page_name, sentence_id in page_sent_id_pairs:\n",
    "                text = mapping[page][sentence_id]\n",
    "                if text != \"\":\n",
    "                    claims.append(claim)\n",
    "                    sentences.append(text.replace(\" \",\"\"))\n",
    "                    idx.append(page)\n",
    "                    if not is_testset:\n",
    "                        evidence.append(df[\"evidence\"].iloc[i])\n",
    "                    predicted_evidence.append([page_name, int(sentence_id)])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"claim\": claims,\n",
    "        \"text\": sentences,\n",
    "        \"idx\": idx,\n",
    "        \"evidence\": evidence if not is_testset else None,\n",
    "        \"predicted_evidence\": predicted_evidence,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Setup training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title  { display-mode: \"form\" }\n",
    "TOP_N = 5  #@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation part (15 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Check on our test data\n",
    "(5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "Load pretrained checkpoint: /workspace/main/AICUP-2023-Truth-main/step2_new_upload/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "predictor = MultiModalPredictor.load(step2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "lVFusJqjmex-",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>text</th>\n",
       "      <th>idx</th>\n",
       "      <th>evidence</th>\n",
       "      <th>predicted_evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>光學顯微鏡（Opticalmicroscope、Lightmicroscope）是一種利用光...</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>由物體入射的光被至少兩個光學系統（物鏡和目鏡）放大。</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>首先物鏡產生一個被放大實像，人眼通過作用相當於放大鏡的目鏡觀察這個已經被放大了的實像。</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>一般的光學顯微鏡有多個可以替換的物鏡，這樣觀察者可以按需要更換放大倍數，也就是增加放大倍率，...</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>這些物鏡一般被安置在一個可以轉動的物鏡盤上，轉動物鏡盤就可以使不同的物鏡方便地進入光路，物鏡...</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 claim  \\\n",
       "0  光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "1  光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "2  光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "3  光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "4  光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "\n",
       "                                                text    idx evidence  \\\n",
       "0  光學顯微鏡（Opticalmicroscope、Lightmicroscope）是一種利用光...  光學顯微鏡     None   \n",
       "1                         由物體入射的光被至少兩個光學系統（物鏡和目鏡）放大。  光學顯微鏡     None   \n",
       "2        首先物鏡產生一個被放大實像，人眼通過作用相當於放大鏡的目鏡觀察這個已經被放大了的實像。  光學顯微鏡     None   \n",
       "3  一般的光學顯微鏡有多個可以替換的物鏡，這樣觀察者可以按需要更換放大倍數，也就是增加放大倍率，...  光學顯微鏡     None   \n",
       "4  這些物鏡一般被安置在一個可以轉動的物鏡盤上，轉動物鏡盤就可以使不同的物鏡方便地進入光路，物鏡...  光學顯微鏡     None   \n",
       "\n",
       "  predicted_evidence  \n",
       "0         [光學顯微鏡, 0]  \n",
       "1         [光學顯微鏡, 3]  \n",
       "2         [光學顯微鏡, 4]  \n",
       "3         [光學顯微鏡, 5]  \n",
       "4         [光學顯微鏡, 6]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = load_json(\"data/test_doc5.jsonl\")\n",
    "\n",
    "test_evidences = pair_with_wiki_sentences_eval(\n",
    "    mapping,\n",
    "    pd.DataFrame(test_data),\n",
    "    is_testset=True,\n",
    ")\n",
    "test_evidences.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting the test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fe841efc18478ca0af1d2bdb7ababa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Start predicting the test data\")\n",
    "probs = predictor.predict_proba(test_evidences)\n",
    "probs = probs.to_numpy()\n",
    "second_values = [p[1] for p in probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_retrieval(\n",
    "    probs=second_values,\n",
    "    df_evidences=test_evidences,\n",
    "    ground_truths=test_data,\n",
    "    top_n=TOP_N,\n",
    "    cal_scores=False,\n",
    "    save_name=f\"test_doc5sent{TOP_N}.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>text</th>\n",
       "      <th>idx</th>\n",
       "      <th>evidence</th>\n",
       "      <th>predicted_evidence</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>光學顯微鏡（Opticalmicroscope、Lightmicroscope）是一種利用光...</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 0]</td>\n",
       "      <td>0.964085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>由物體入射的光被至少兩個光學系統（物鏡和目鏡）放大。</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 3]</td>\n",
       "      <td>0.221519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>首先物鏡產生一個被放大實像，人眼通過作用相當於放大鏡的目鏡觀察這個已經被放大了的實像。</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 4]</td>\n",
       "      <td>0.009423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>一般的光學顯微鏡有多個可以替換的物鏡，這樣觀察者可以按需要更換放大倍數，也就是增加放大倍率，...</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 5]</td>\n",
       "      <td>0.001495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>這些物鏡一般被安置在一個可以轉動的物鏡盤上，轉動物鏡盤就可以使不同的物鏡方便地進入光路，物鏡...</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 6]</td>\n",
       "      <td>0.000617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>十八世紀，光學顯微鏡的放大倍率已經提高到了1000倍，使人們能用眼睛看清微生物體的形態、大小...</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 9]</td>\n",
       "      <td>0.086170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>直到物理學家發現了放大倍率與解析度之間的規律，人們才知道光學顯微鏡的解析度是有極限的，解析度...</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 10]</td>\n",
       "      <td>0.001619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>光學顯微鏡的解析度受到光波長的限制，一般不超過0.3微米。</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 11]</td>\n",
       "      <td>0.002479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>假如顯微鏡使用紫外線作爲光源或物體被放在油中的話，解析度還可以得到提高。</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 12]</td>\n",
       "      <td>0.000652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>光學顯微鏡依樣品的不同可分爲反射式和透射式。</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 15]</td>\n",
       "      <td>0.001647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>反射顯微鏡的物體一般是不透明的，光從上面照在物體上，被物體反射的光進入顯微鏡。</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 16]</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>這種顯微鏡經常被用來觀察固體等，多應用在工學、材料領域，在正立顯微鏡中，此類顯微鏡又稱作金相...</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 17]</td>\n",
       "      <td>0.085327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>透射顯微鏡的物體是透明的或非常薄，光從可透過它進入顯微鏡。</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 18]</td>\n",
       "      <td>0.002327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>這種顯微鏡常被用來觀察生物組織。</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 19]</td>\n",
       "      <td>0.046249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>光學顯微鏡依其聚光鏡（condenser）和物鏡（Objective）的設計，可用來觀察不同...</td>\n",
       "      <td>光學顯微鏡</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 22]</td>\n",
       "      <td>0.014396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  claim  \\\n",
       "0   光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "1   光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "2   光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "3   光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "4   光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "5   光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "6   光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "7   光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "8   光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "9   光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "10  光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "11  光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "12  光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "13  光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "14  光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "\n",
       "                                                 text    idx evidence  \\\n",
       "0   光學顯微鏡（Opticalmicroscope、Lightmicroscope）是一種利用光...  光學顯微鏡     None   \n",
       "1                          由物體入射的光被至少兩個光學系統（物鏡和目鏡）放大。  光學顯微鏡     None   \n",
       "2         首先物鏡產生一個被放大實像，人眼通過作用相當於放大鏡的目鏡觀察這個已經被放大了的實像。  光學顯微鏡     None   \n",
       "3   一般的光學顯微鏡有多個可以替換的物鏡，這樣觀察者可以按需要更換放大倍數，也就是增加放大倍率，...  光學顯微鏡     None   \n",
       "4   這些物鏡一般被安置在一個可以轉動的物鏡盤上，轉動物鏡盤就可以使不同的物鏡方便地進入光路，物鏡...  光學顯微鏡     None   \n",
       "5   十八世紀，光學顯微鏡的放大倍率已經提高到了1000倍，使人們能用眼睛看清微生物體的形態、大小...  光學顯微鏡     None   \n",
       "6   直到物理學家發現了放大倍率與解析度之間的規律，人們才知道光學顯微鏡的解析度是有極限的，解析度...  光學顯微鏡     None   \n",
       "7                       光學顯微鏡的解析度受到光波長的限制，一般不超過0.3微米。  光學顯微鏡     None   \n",
       "8                假如顯微鏡使用紫外線作爲光源或物體被放在油中的話，解析度還可以得到提高。  光學顯微鏡     None   \n",
       "9                              光學顯微鏡依樣品的不同可分爲反射式和透射式。  光學顯微鏡     None   \n",
       "10            反射顯微鏡的物體一般是不透明的，光從上面照在物體上，被物體反射的光進入顯微鏡。  光學顯微鏡     None   \n",
       "11  這種顯微鏡經常被用來觀察固體等，多應用在工學、材料領域，在正立顯微鏡中，此類顯微鏡又稱作金相...  光學顯微鏡     None   \n",
       "12                      透射顯微鏡的物體是透明的或非常薄，光從可透過它進入顯微鏡。  光學顯微鏡     None   \n",
       "13                                   這種顯微鏡常被用來觀察生物組織。  光學顯微鏡     None   \n",
       "14  光學顯微鏡依其聚光鏡（condenser）和物鏡（Objective）的設計，可用來觀察不同...  光學顯微鏡     None   \n",
       "\n",
       "   predicted_evidence      prob  \n",
       "0          [光學顯微鏡, 0]  0.964085  \n",
       "1          [光學顯微鏡, 3]  0.221519  \n",
       "2          [光學顯微鏡, 4]  0.009423  \n",
       "3          [光學顯微鏡, 5]  0.001495  \n",
       "4          [光學顯微鏡, 6]  0.000617  \n",
       "5          [光學顯微鏡, 9]  0.086170  \n",
       "6         [光學顯微鏡, 10]  0.001619  \n",
       "7         [光學顯微鏡, 11]  0.002479  \n",
       "8         [光學顯微鏡, 12]  0.000652  \n",
       "9         [光學顯微鏡, 15]  0.001647  \n",
       "10        [光學顯微鏡, 16]  0.000804  \n",
       "11        [光學顯微鏡, 17]  0.085327  \n",
       "12        [光學顯微鏡, 18]  0.002327  \n",
       "13        [光學顯微鏡, 19]  0.046249  \n",
       "14        [光學顯微鏡, 22]  0.014396  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evidences.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGzl8a5JteT7",
    "tags": []
   },
   "source": [
    "notebook3\n",
    "## PART 3. Claim verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tgA1vcUyzjlx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils import (\n",
    "    generate_evidence_to_wiki_pages_mapping,\n",
    "    jsonl_dir_to_df,\n",
    "    load_json,\n",
    ")\n",
    "\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload wiki database (same as part 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and concatenating jsonl files in data/wiki-pages\n",
      "Generate parse mapping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa54f29255a421bb2f134e33037b4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=296938), Label(value='0 / 296938')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform to id to evidence_map mapping\n"
     ]
    }
   ],
   "source": [
    "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages,)\n",
    "del wiki_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_pages\n"
     ]
    }
   ],
   "source": [
    "print(\"wiki_pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def join_with_topk_evidence(\n",
    "    df: pd.DataFrame,\n",
    "    mapping: dict,\n",
    "    mode: str = \"train\",\n",
    "    topk: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"join_with_topk_evidence join the dataset with topk evidence.\n",
    "\n",
    "    Note:\n",
    "        After extraction, the dataset will be like this:\n",
    "               id     label         claim                           evidence            evidence_list\n",
    "        0    4604  supports       高行健...     [[[3393, 3552, 高行健, 0], [...  [高行健 （ ）江西赣州出...\n",
    "        ..    ...       ...            ...                                ...                     ...\n",
    "        945  2095  supports       美國總...  [[[1879, 2032, 吉米·卡特, 16], [...  [卸任后 ， 卡特積極參與...\n",
    "        停各种战争及人質危機的斡旋工作 ， 反对美国小布什政府攻打伊拉克...\n",
    "\n",
    "        [946 rows x 5 columns]\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset with evidence.\n",
    "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
    "        topk (int, optional): The topk evidence. Defaults to 5.\n",
    "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
    "            If cache is None, return the result directly.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with topk evidence_list.\n",
    "            The `evidence_list` column will be: List[str]\n",
    "    \"\"\"\n",
    "\n",
    "    # format evidence column to List[List[Tuple[str, str, str, str]]]\n",
    "    if \"evidence\" in df.columns:\n",
    "        df[\"evidence\"] = df[\"evidence\"].parallel_map(\n",
    "            lambda x: [[x]] if not isinstance(x[0], list) else [x]\n",
    "            if not isinstance(x[0][0], list) else x)\n",
    "\n",
    "    print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
    "    df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(lambda x: [\n",
    "            mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "            for evi_id, evi_idx in x  # for each evidence list\n",
    "        ][:topk] if isinstance(x, list) else [])\n",
    "    print(df[\"evidence_list\"][:5])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Setup training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title  { display-mode: \"form\" }\n",
    "\n",
    "EVIDENCE_TOPK = 5  #@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Concat claim and evidences\n",
    "join topk evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prevent CUDA out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training (30 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_to_string(lst):\n",
    "    # 將 list 轉換為 string，並刪除所有空格\n",
    "    return \"[PAD]\".join(str(item).replace(' ', '') for item in lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Make your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "zLkfuoAE49mz",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "Load pretrained checkpoint: /workspace/main/AICUP-2023-Truth-main/step3_base_upload/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting evidence_list for the eval mode ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034d8852e80c403c84fa1cfff3f9aa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2260), Label(value='0 / 2260'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [光學顯微鏡 （ Optical microscope 、 Light microscope...\n",
      "1    [家蠶 （ 學名 ： Bombyx mori ） 是鱗翅目蠶蛾科家蠶蛾屬的完全變態昆蟲 ， ...\n",
      "2    [綠山城縣  ， 是波蘭的縣份 ， 位於該國西部 ， 由盧布斯卡省負責管轄 ， 首府設於綠山...\n",
      "3    [《 魂斷藍橋 》 （ Waterloo Bridge ） 是美國黑白電影 ， 由米高梅電影...\n",
      "4    [2015年以 《 刺客聶隱娘 》 獲得第68屆坎城影展最佳導演獎及第52屆金馬獎最佳導演獎...\n",
      "Name: evidence_list, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "predictor = MultiModalPredictor.load(step3_model)\n",
    "TEST_DATA = load_json(\"data/test_doc5sent5.jsonl\")\n",
    "TEST_PKL_FILE = Path(\"data/test_doc5sent5.pkl\")\n",
    "\n",
    "test_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(TEST_DATA),\n",
    "        mapping,\n",
    "        mode=\"eval\",\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "test_df.to_pickle(TEST_PKL_FILE, protocol=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>claim</th>\n",
       "      <th>predicted_pages</th>\n",
       "      <th>predicted_evidence</th>\n",
       "      <th>evidence_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5208</td>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>[光學顯微鏡, 物, 顯微鏡, 肉眼, 電磁學]</td>\n",
       "      <td>[[光學顯微鏡, 0], [顯微鏡, 0], [光學顯微鏡, 3], [光學顯微鏡, 9],...</td>\n",
       "      <td>[光學顯微鏡 （ Optical microscope 、 Light microscope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1019</td>\n",
       "      <td>產絲的蠶或產蜜的蜜蜂爲提供間接經濟利益的昆蟲。</td>\n",
       "      <td>[蠶, 昆蟲]</td>\n",
       "      <td>[[蠶, 0], [昆蟲, 22], [昆蟲, 23], [蠶, 4], [蠶, 5]]</td>\n",
       "      <td>[家蠶 （ 學名 ： Bombyx mori ） 是鱗翅目蠶蛾科家蠶蛾屬的完全變態昆蟲 ， ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8514</td>\n",
       "      <td>波蘭西部的綠山城縣平均每平方公里的土地有0人。</td>\n",
       "      <td>[綠山城縣, 人, 0, 波蘭, 土地, 西部]</td>\n",
       "      <td>[[綠山城縣, 0], [波蘭, 1], [波蘭, 0], [土地, 10], [土地, 8]]</td>\n",
       "      <td>[綠山城縣  ， 是波蘭的縣份 ， 位於該國西部 ， 由盧布斯卡省負責管轄 ， 首府設於綠山...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1874</td>\n",
       "      <td>VivienLeigh主演魂斷藍橋中的女配角。</td>\n",
       "      <td>[配角, 藍橋, 橋, 魂斷藍橋]</td>\n",
       "      <td>[[魂斷藍橋, 0], [藍橋, 2], [藍橋, 1], [魂斷藍橋, 6], [魂斷藍橋...</td>\n",
       "      <td>[《 魂斷藍橋 》 （ Waterloo Bridge ） 是美國黑白電影 ， 由米高梅電影...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8352</td>\n",
       "      <td>侯孝賢改編自唐代文言文學的電影獲得金馬獎最佳劇情片獎。</td>\n",
       "      <td>[劇情片, 文學, 文言文, 金馬獎, 電影, 侯孝賢]</td>\n",
       "      <td>[[侯孝賢, 2], [侯孝賢, 1], [侯孝賢, 0], [侯孝賢, 3], [侯孝賢,...</td>\n",
       "      <td>[2015年以 《 刺客聶隱娘 》 獲得第68屆坎城影展最佳導演獎及第52屆金馬獎最佳導演獎...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9033</th>\n",
       "      <td>12412</td>\n",
       "      <td>劉德華在中國星集團拍攝的最後一部電影爲大隻佬_(電影），之後他就結束與中國星集團14年賓主關係。</td>\n",
       "      <td>[中國星集團, 電影, 佬, 大隻佬_(電影), 劉德華]</td>\n",
       "      <td>[[大隻佬_(電影), 8], [大隻佬_(電影), 0], [大隻佬_(電影), 5], ...</td>\n",
       "      <td>[該片是劉德華在中國星集團拍攝的最後一部電影 ， 之後便結束與中國星集團的14年賓主關係 ，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9034</th>\n",
       "      <td>9611</td>\n",
       "      <td>視光學是利用光學儀器搭配侵入的方式來治療患者雙眼視覺異常。</td>\n",
       "      <td>[患者, 眼, 雙眼視覺, 視光學, 光學儀器, 光學]</td>\n",
       "      <td>[[視光學, 0], [視光學, 1], [雙眼視覺, 18], [眼, 4], [光學, 0]]</td>\n",
       "      <td>[視光學 （ 英文 ： Optometry ） ， 是光學和眼科的結合 ， 運用光學儀器來檢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9035</th>\n",
       "      <td>10538</td>\n",
       "      <td>婊子千層麪這首歌諷刺了印度音樂唱片公司「TSeries」，用來回應TSeries在訂閱數即將...</td>\n",
       "      <td>[PewDiePie, 婊子千層麪]</td>\n",
       "      <td>[[婊子千層麪, 1], [婊子千層麪, 6], [婊子千層麪, 2], [婊子千層麪, 0...</td>\n",
       "      <td>[這首歌諷刺了印度音樂唱片公司 「 T - Series 」 ， 以回應T - Series...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9036</th>\n",
       "      <td>11757</td>\n",
       "      <td>兩棲動物由於在水中適應比不上腔棘魚，在陸地生存又落後羊膜動物，因此現今大多種類已絕種。</td>\n",
       "      <td>[兩棲動物, 水, 陸地, 腔棘魚, 魚, 動物, 羊膜, 羊膜動物, 腔]</td>\n",
       "      <td>[[兩棲動物, 7], [兩棲動物, 8], [兩棲動物, 3], [兩棲動物, 6], [...</td>\n",
       "      <td>[早期兩棲動物在石炭紀繁盛一時 ， 分化出許多大型種類 ， 爲淡水和陸地上的頂級捕食者 ， ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9037</th>\n",
       "      <td>504</td>\n",
       "      <td>鐒是由阿伯特·吉奧索等人發現的。</td>\n",
       "      <td>[人]</td>\n",
       "      <td>[[人, 1], [人, 12], [人, 15], [人, 4], [人, 10]]</td>\n",
       "      <td>[長者智人化石表明 ， 現代人類在約20萬年前的東非大裂谷演化成形 。, 人尤其擅長用口語 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9038 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              claim  \\\n",
       "0      5208                光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "1      1019                            產絲的蠶或產蜜的蜜蜂爲提供間接經濟利益的昆蟲。   \n",
       "2      8514                            波蘭西部的綠山城縣平均每平方公里的土地有0人。   \n",
       "3      1874                            VivienLeigh主演魂斷藍橋中的女配角。   \n",
       "4      8352                        侯孝賢改編自唐代文言文學的電影獲得金馬獎最佳劇情片獎。   \n",
       "...     ...                                                ...   \n",
       "9033  12412   劉德華在中國星集團拍攝的最後一部電影爲大隻佬_(電影），之後他就結束與中國星集團14年賓主關係。   \n",
       "9034   9611                      視光學是利用光學儀器搭配侵入的方式來治療患者雙眼視覺異常。   \n",
       "9035  10538  婊子千層麪這首歌諷刺了印度音樂唱片公司「TSeries」，用來回應TSeries在訂閱數即將...   \n",
       "9036  11757        兩棲動物由於在水中適應比不上腔棘魚，在陸地生存又落後羊膜動物，因此現今大多種類已絕種。   \n",
       "9037    504                                   鐒是由阿伯特·吉奧索等人發現的。   \n",
       "\n",
       "                             predicted_pages  \\\n",
       "0                   [光學顯微鏡, 物, 顯微鏡, 肉眼, 電磁學]   \n",
       "1                                    [蠶, 昆蟲]   \n",
       "2                   [綠山城縣, 人, 0, 波蘭, 土地, 西部]   \n",
       "3                          [配角, 藍橋, 橋, 魂斷藍橋]   \n",
       "4               [劇情片, 文學, 文言文, 金馬獎, 電影, 侯孝賢]   \n",
       "...                                      ...   \n",
       "9033           [中國星集團, 電影, 佬, 大隻佬_(電影), 劉德華]   \n",
       "9034            [患者, 眼, 雙眼視覺, 視光學, 光學儀器, 光學]   \n",
       "9035                      [PewDiePie, 婊子千層麪]   \n",
       "9036  [兩棲動物, 水, 陸地, 腔棘魚, 魚, 動物, 羊膜, 羊膜動物, 腔]   \n",
       "9037                                     [人]   \n",
       "\n",
       "                                     predicted_evidence  \\\n",
       "0     [[光學顯微鏡, 0], [顯微鏡, 0], [光學顯微鏡, 3], [光學顯微鏡, 9],...   \n",
       "1          [[蠶, 0], [昆蟲, 22], [昆蟲, 23], [蠶, 4], [蠶, 5]]   \n",
       "2      [[綠山城縣, 0], [波蘭, 1], [波蘭, 0], [土地, 10], [土地, 8]]   \n",
       "3     [[魂斷藍橋, 0], [藍橋, 2], [藍橋, 1], [魂斷藍橋, 6], [魂斷藍橋...   \n",
       "4     [[侯孝賢, 2], [侯孝賢, 1], [侯孝賢, 0], [侯孝賢, 3], [侯孝賢,...   \n",
       "...                                                 ...   \n",
       "9033  [[大隻佬_(電影), 8], [大隻佬_(電影), 0], [大隻佬_(電影), 5], ...   \n",
       "9034  [[視光學, 0], [視光學, 1], [雙眼視覺, 18], [眼, 4], [光學, 0]]   \n",
       "9035  [[婊子千層麪, 1], [婊子千層麪, 6], [婊子千層麪, 2], [婊子千層麪, 0...   \n",
       "9036  [[兩棲動物, 7], [兩棲動物, 8], [兩棲動物, 3], [兩棲動物, 6], [...   \n",
       "9037        [[人, 1], [人, 12], [人, 15], [人, 4], [人, 10]]   \n",
       "\n",
       "                                          evidence_list  \n",
       "0     [光學顯微鏡 （ Optical microscope 、 Light microscope...  \n",
       "1     [家蠶 （ 學名 ： Bombyx mori ） 是鱗翅目蠶蛾科家蠶蛾屬的完全變態昆蟲 ， ...  \n",
       "2     [綠山城縣  ， 是波蘭的縣份 ， 位於該國西部 ， 由盧布斯卡省負責管轄 ， 首府設於綠山...  \n",
       "3     [《 魂斷藍橋 》 （ Waterloo Bridge ） 是美國黑白電影 ， 由米高梅電影...  \n",
       "4     [2015年以 《 刺客聶隱娘 》 獲得第68屆坎城影展最佳導演獎及第52屆金馬獎最佳導演獎...  \n",
       "...                                                 ...  \n",
       "9033  [該片是劉德華在中國星集團拍攝的最後一部電影 ， 之後便結束與中國星集團的14年賓主關係 ，...  \n",
       "9034  [視光學 （ 英文 ： Optometry ） ， 是光學和眼科的結合 ， 運用光學儀器來檢...  \n",
       "9035  [這首歌諷刺了印度音樂唱片公司 「 T - Series 」 ， 以回應T - Series...  \n",
       "9036  [早期兩棲動物在石炭紀繁盛一時 ， 分化出許多大型種類 ， 爲淡水和陸地上的頂級捕食者 ， ...  \n",
       "9037  [長者智人化石表明 ， 現代人類在約20萬年前的東非大裂谷演化成形 。, 人尤其擅長用口語 ...  \n",
       "\n",
       "[9038 rows x 5 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_org = test_df.copy()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1965/839020936.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['evidence_list'] = test_df['evidence_list'].apply(list_to_string)\n"
     ]
    }
   ],
   "source": [
    "test_df = test_df[[\"claim\",\"evidence_list\"]]\n",
    "test_df['evidence_list'] = test_df['evidence_list'].apply(list_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>光學顯微鏡（Opticalmicroscope、Lightmicroscope）是一種利用光...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>產絲的蠶或產蜜的蜜蜂爲提供間接經濟利益的昆蟲。</td>\n",
       "      <td>家蠶（學名：Bombyxmori）是鱗翅目蠶蛾科家蠶蛾屬的完全變態昆蟲，爲絲綢的主要原料來源...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>波蘭西部的綠山城縣平均每平方公里的土地有0人。</td>\n",
       "      <td>綠山城縣，是波蘭的縣份，位於該國西部，由盧布斯卡省負責管轄，首府設於綠山城，面積1,571平...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VivienLeigh主演魂斷藍橋中的女配角。</td>\n",
       "      <td>《魂斷藍橋》（WaterlooBridge）是美國黑白電影，由米高梅電影公司於1940年出品...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>侯孝賢改編自唐代文言文學的電影獲得金馬獎最佳劇情片獎。</td>\n",
       "      <td>2015年以《刺客聶隱娘》獲得第68屆坎城影展最佳導演獎及第52屆金馬獎最佳導演獎與金馬獎最...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9033</th>\n",
       "      <td>劉德華在中國星集團拍攝的最後一部電影爲大隻佬_(電影），之後他就結束與中國星集團14年賓主關係。</td>\n",
       "      <td>該片是劉德華在中國星集團拍攝的最後一部電影，之後便結束與中國星集團的14年賓主關係，加盟寰亞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9034</th>\n",
       "      <td>視光學是利用光學儀器搭配侵入的方式來治療患者雙眼視覺異常。</td>\n",
       "      <td>視光學（英文：Optometry），是光學和眼科的結合，運用光學儀器來檢查眼睛的視覺功能，並...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9035</th>\n",
       "      <td>婊子千層麪這首歌諷刺了印度音樂唱片公司「TSeries」，用來回應TSeries在訂閱數即將...</td>\n",
       "      <td>這首歌諷刺了印度音樂唱片公司「T-Series」，以回應T-Series在訂閱數即將超過Pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9036</th>\n",
       "      <td>兩棲動物由於在水中適應比不上腔棘魚，在陸地生存又落後羊膜動物，因此現今大多種類已絕種。</td>\n",
       "      <td>早期兩棲動物在石炭紀繁盛一時，分化出許多大型種類，爲淡水和陸地上的頂級捕食者，但由於食性較爲...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9037</th>\n",
       "      <td>鐒是由阿伯特·吉奧索等人發現的。</td>\n",
       "      <td>長者智人化石表明，現代人類在約20萬年前的東非大裂谷演化成形。[PAD]人尤其擅長用口語、手...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9038 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  claim  \\\n",
       "0                   光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "1                               產絲的蠶或產蜜的蜜蜂爲提供間接經濟利益的昆蟲。   \n",
       "2                               波蘭西部的綠山城縣平均每平方公里的土地有0人。   \n",
       "3                               VivienLeigh主演魂斷藍橋中的女配角。   \n",
       "4                           侯孝賢改編自唐代文言文學的電影獲得金馬獎最佳劇情片獎。   \n",
       "...                                                 ...   \n",
       "9033   劉德華在中國星集團拍攝的最後一部電影爲大隻佬_(電影），之後他就結束與中國星集團14年賓主關係。   \n",
       "9034                      視光學是利用光學儀器搭配侵入的方式來治療患者雙眼視覺異常。   \n",
       "9035  婊子千層麪這首歌諷刺了印度音樂唱片公司「TSeries」，用來回應TSeries在訂閱數即將...   \n",
       "9036        兩棲動物由於在水中適應比不上腔棘魚，在陸地生存又落後羊膜動物，因此現今大多種類已絕種。   \n",
       "9037                                   鐒是由阿伯特·吉奧索等人發現的。   \n",
       "\n",
       "                                          evidence_list  \n",
       "0     光學顯微鏡（Opticalmicroscope、Lightmicroscope）是一種利用光...  \n",
       "1     家蠶（學名：Bombyxmori）是鱗翅目蠶蛾科家蠶蛾屬的完全變態昆蟲，爲絲綢的主要原料來源...  \n",
       "2     綠山城縣，是波蘭的縣份，位於該國西部，由盧布斯卡省負責管轄，首府設於綠山城，面積1,571平...  \n",
       "3     《魂斷藍橋》（WaterlooBridge）是美國黑白電影，由米高梅電影公司於1940年出品...  \n",
       "4     2015年以《刺客聶隱娘》獲得第68屆坎城影展最佳導演獎及第52屆金馬獎最佳導演獎與金馬獎最...  \n",
       "...                                                 ...  \n",
       "9033  該片是劉德華在中國星集團拍攝的最後一部電影，之後便結束與中國星集團的14年賓主關係，加盟寰亞...  \n",
       "9034  視光學（英文：Optometry），是光學和眼科的結合，運用光學儀器來檢查眼睛的視覺功能，並...  \n",
       "9035  這首歌諷刺了印度音樂唱片公司「T-Series」，以回應T-Series在訂閱數即將超過Pe...  \n",
       "9036  早期兩棲動物在石炭紀繁盛一時，分化出許多大型種類，爲淡水和陸地上的頂級捕食者，但由於食性較爲...  \n",
       "9037  長者智人化石表明，現代人類在約20萬年前的東非大裂谷演化成形。[PAD]人尤其擅長用口語、手...  \n",
       "\n",
       "[9038 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3707f7b043c49999213a58225062af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_label = predictor.predict(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_dataset = test_df_org\n",
    "predict_dataset[\"predicted_label\"] = predicted_label\n",
    "OUTPUT_FILENAME = \"submission.jsonl\"\n",
    "predict_dataset[[\"id\", \"predicted_label\", \"predicted_evidence\"]].to_json(\n",
    "    OUTPUT_FILENAME,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將預測 SUPPORTS 或是 REFUTES 但 predicted_evidence 是空的更改成 NOT ENOUGH INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 533, 'predicted_label': 'supports', 'predicted_evidence': []}\n",
      "{'id': 13926, 'predicted_label': 'supports', 'predicted_evidence': []}\n",
      "{'id': 526, 'predicted_label': 'supports', 'predicted_evidence': []}\n",
      "{'id': 17223, 'predicted_label': 'refutes', 'predicted_evidence': []}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from utils import load_json\n",
    "\n",
    "data = load_json(OUTPUT_FILENAME)\n",
    "# data predicted_evidence is 0 change this label\n",
    "for item in data:\n",
    "    if item['predicted_label'] == 'NOT ENOUGH INFO':\n",
    "        continue\n",
    "    length = len(item['predicted_evidence'])\n",
    "    if length == 0:\n",
    "        print(item)\n",
    "        item['predicted_label'] = 'NOT ENOUGH INFO'\n",
    "# save this json \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_json(OUTPUT_FILENAME+\"_Del\", orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "10286f3c74912972f7d1fdceceee5be5b7c77248e5efe5afcbc6a71f24d230fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

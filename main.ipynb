{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "python: 3.8.*\n",
    "\n",
    "use ```Ctrl + ]``` to collapse all section :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download our starter pack (3~5 min)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "h7MSEcenjVrL"
   },
   "source": [
    "notebook1\n",
    "## PART 1. Document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencc in /opt/conda/lib/python3.9/site-packages (1.1.6)\n",
      "Requirement already satisfied: wikipedia in /opt/conda/lib/python3.9/site-packages (1.4.0)\n",
      "Requirement already satisfied: pandarallel in /opt/conda/lib/python3.9/site-packages (1.6.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.9/site-packages (from wikipedia) (4.11.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wikipedia) (2.28.2)\n",
      "Requirement already satisfied: dill>=0.3.1 in /opt/conda/lib/python3.9/site-packages (from pandarallel) (0.3.6)\n",
      "Requirement already satisfied: pandas>=1 in /opt/conda/lib/python3.9/site-packages (from pandarallel) (1.5.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from pandarallel) (5.9.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1->pandarallel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1->pandarallel) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=1->pandarallel) (1.23.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.9/site-packages (from beautifulsoup4->wikipedia) (2.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1->pandarallel) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install opencc wikipedia pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hanlp in /opt/conda/lib/python3.9/site-packages (2.1.0b49)\n",
      "Requirement already satisfied: hanlp-common>=0.0.19 in /opt/conda/lib/python3.9/site-packages (from hanlp) (0.0.19)\n",
      "Requirement already satisfied: hanlp-downloader in /opt/conda/lib/python3.9/site-packages (from hanlp) (0.0.25)\n",
      "Requirement already satisfied: hanlp-trie>=0.0.4 in /opt/conda/lib/python3.9/site-packages (from hanlp) (0.0.5)\n",
      "Requirement already satisfied: pynvml in /opt/conda/lib/python3.9/site-packages (from hanlp) (11.5.0)\n",
      "Requirement already satisfied: sentencepiece>=0.1.91 in /opt/conda/lib/python3.9/site-packages (from hanlp) (0.1.97)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.9/site-packages (from hanlp) (2.3.0)\n",
      "Requirement already satisfied: tokenizers==0.11.6 in /opt/conda/lib/python3.9/site-packages (from hanlp) (0.11.6)\n",
      "Requirement already satisfied: toposort==1.5 in /opt/conda/lib/python3.9/site-packages (from hanlp) (1.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from hanlp) (1.13.1+cu117)\n",
      "Requirement already satisfied: transformers>=4.1.1 in /opt/conda/lib/python3.9/site-packages (from hanlp) (4.27.1)\n",
      "Requirement already satisfied: phrasetree in /opt/conda/lib/python3.9/site-packages (from hanlp-common>=0.0.19->hanlp) (0.0.8)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.6.0->hanlp) (4.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers>=4.1.1->hanlp) (3.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.1.1->hanlp) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.1.1->hanlp) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.1.1->hanlp) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.1.1->hanlp) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.1.1->hanlp) (2022.10.31)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers>=4.1.1->hanlp) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.1.1->hanlp) (4.64.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.1.1->hanlp) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.1.1->hanlp) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.1.1->hanlp) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.1.1->hanlp) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install hanlp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the environment and import all library we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "niqu9pLajYC_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# built-in libs\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "\n",
    "# 3rd party libs\n",
    "import hanlp\n",
    "import opencc\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "from hanlp.components.pipeline import Pipeline\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "# our own libs\n",
    "\n",
    "\n",
    "def load_json(file_path: Union[Path, str]) -> pd.DataFrame:\n",
    "    \"\"\"jsonl_to_df read jsonl file and return a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path (Union[Path, str]): The jsonl file path.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The jsonl file content.\n",
    "\n",
    "    Example:\n",
    "        >>> read_jsonl_file(\"data/train.jsonl\")\n",
    "               id            label  ... predicted_label                                      evidence_list\n",
    "        0    3984          refutes  ...         REFUTES  [城市規劃是城市建設及管理的依據 ， 位於城市管理之規劃 、 建設 、 運作三個階段之首 ，...\n",
    "        ..    ...              ...  ...             ...                                                ...\n",
    "        945  3042         supports  ...         REFUTES  [北歐人相傳每當雷雨交加時就是索爾乘坐馬車出來巡視 ， 因此稱呼索爾為 “ 雷神 ” 。, ...\n",
    "\n",
    "        [946 rows x 10 columns]\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf8\") as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    return [json.loads(json_str) for json_str in json_list]\n",
    "\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)\n",
    "wikipedia.set_lang(\"zh\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = load_json(\"data/public_train.jsonl\")\n",
    "TEST_DATA = load_json(\"data/public_test.jsonl\")\n",
    "CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
    "CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data class for type hinting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Claim:\n",
    "    data: str\n",
    "\n",
    "@dataclass\n",
    "class AnnotationID:\n",
    "    id: int\n",
    "\n",
    "@dataclass\n",
    "class EvidenceID:\n",
    "    id: int\n",
    "\n",
    "@dataclass\n",
    "class PageTitle:\n",
    "    title: str\n",
    "\n",
    "@dataclass\n",
    "class SentenceID:\n",
    "    id: int\n",
    "\n",
    "@dataclass\n",
    "class Evidence:\n",
    "    data: List[List[Tuple[AnnotationID, EvidenceID, PageTitle, SentenceID]]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of consistency, we convert traditional to simplified Chinese first before converting it back to traditional Chinese.  This is due to some errors occuring when converting traditional to traditional Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "A3NU01DnjKp-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_st_corrections(text: str) -> str:\n",
    "    simplified = CONVERTER_T2S.convert(text)\n",
    "\n",
    "    return CONVERTER_S2T.convert(simplified)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use constituency parsing to separate part of speeches or so called constituent to extract noun phrases.  In the later stages, we will use the noun phrases as the query to search for relevant documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nps_hanlp(\n",
    "    predictor: Pipeline,\n",
    "    d: Dict[str, Union[int, Claim, Evidence]],\n",
    ") -> List[str]:\n",
    "    claim = d[\"claim\"]\n",
    "    tree = predictor(claim)[\"con\"]\n",
    "    nps = [\n",
    "        do_st_corrections(\"\".join(subtree.leaves()))\n",
    "        for subtree in tree.subtrees(lambda t: t.label() == \"NP\")\n",
    "    ]\n",
    "\n",
    "    return nps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision refers to how many related documents are retrieved.  Recall refers to how many relevant documents are retrieved.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_precision(\n",
    "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "    predictions: pd.Series,\n",
    ") -> None:\n",
    "    precision = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        # Extract all ground truth of titles of the wikipedia pages\n",
    "        # evidence[2] refers to the title of the wikipedia page\n",
    "        gt_pages = set([\n",
    "            evidence[2]\n",
    "            for evidence_set in d[\"evidence\"]\n",
    "            for evidence in evidence_set\n",
    "        ])\n",
    "\n",
    "        predicted_pages = predictions.iloc[i]\n",
    "        hits = predicted_pages.intersection(gt_pages)\n",
    "        if len(predicted_pages) != 0:\n",
    "            precision += len(hits) / len(predicted_pages)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # Macro precision\n",
    "    print(f\"Precision: {precision / count}\")\n",
    "\n",
    "\n",
    "def calculate_recall(\n",
    "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "    predictions: pd.Series,\n",
    ") -> None:\n",
    "    recall = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        gt_pages = set([\n",
    "            evidence[2]\n",
    "            for evidence_set in d[\"evidence\"]\n",
    "            for evidence in evidence_set\n",
    "        ])\n",
    "        predicted_pages = predictions.iloc[i]\n",
    "        hits = predicted_pages.intersection(gt_pages)\n",
    "        recall += len(hits) / len(gt_pages)\n",
    "        count += 1\n",
    "\n",
    "    print(f\"Recall: {recall / count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default amount of documents retrieved is at most five documents.  This `num_pred_doc` can be adjusted based on your objective.  Save data in jsonl format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_doc(\n",
    "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "    predictions: pd.Series,\n",
    "    mode: str = \"train\",\n",
    "    num_pred_doc: int = 5,\n",
    ") -> None:\n",
    "    with open(\n",
    "        f\"data/{mode}_doc{num_pred_doc}.jsonl\",\n",
    "        \"w\",\n",
    "        encoding=\"utf8\",\n",
    "    ) as f:\n",
    "        for i, d in enumerate(data):\n",
    "            d[\"predicted_pages\"] = list(predictions.iloc[i])\n",
    "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function for document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ayGI44qkk_wy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pred_pages(series_data: pd.Series) -> Set[Dict[int, str]]:\n",
    "    results = []\n",
    "    tmp_muji = []\n",
    "    # wiki_page: its index showned in claim\n",
    "    mapping = {}\n",
    "    claim = series_data[\"claim\"]\n",
    "    nps = series_data[\"hanlp_results\"]\n",
    "    first_wiki_term = []\n",
    "\n",
    "    for i, np in enumerate(nps):\n",
    "        # Simplified Traditional Chinese Correction\n",
    "        wiki_search_results = [\n",
    "            do_st_corrections(w) for w in wikipedia.search(np)\n",
    "        ]\n",
    "\n",
    "        # Remove the wiki page's description in brackets\n",
    "        wiki_set = [re.sub(r\"\\s\\(\\S+\\)\", \"\", w) for w in wiki_search_results]\n",
    "        wiki_df = pd.DataFrame({\n",
    "            \"wiki_set\": wiki_set,\n",
    "            \"wiki_results\": wiki_search_results\n",
    "        })\n",
    "\n",
    "        # Elements in wiki_set --> index\n",
    "        # Extracting only the first element is one way to avoid extracting\n",
    "        # too many of the similar wiki pages\n",
    "        grouped_df = wiki_df.groupby(\"wiki_set\", sort=False).first()\n",
    "        candidates = grouped_df[\"wiki_results\"].tolist()\n",
    "        # muji refers to wiki_set\n",
    "        muji = grouped_df.index.tolist()\n",
    "\n",
    "        for prefix, term in zip(muji, candidates):\n",
    "            if prefix not in tmp_muji:\n",
    "                matched = False\n",
    "\n",
    "                # Take at least one term from the first noun phrase\n",
    "                if i == 0:\n",
    "                    first_wiki_term.append(term)\n",
    "\n",
    "                # Walrus operator :=\n",
    "                # https://docs.python.org/3/whatsnew/3.8.html#assignment-expressions\n",
    "                # Through these filters, we are trying to figure out if the term\n",
    "                # is within the claim\n",
    "                if (((new_term := term) in claim) or\n",
    "                    ((new_term := term.replace(\"·\", \"\")) in claim) or\n",
    "                    ((new_term := term.split(\" \")[0]) in claim) or\n",
    "                    ((new_term := term.replace(\"-\", \" \")) in claim)):\n",
    "                    matched = True\n",
    "\n",
    "                elif \"·\" in term:\n",
    "                    splitted = term.split(\"·\")\n",
    "                    for split in splitted:\n",
    "                        if (new_term := split) in claim:\n",
    "                            matched = True\n",
    "                            break\n",
    "\n",
    "                if matched:\n",
    "                    # post-processing\n",
    "                    term = term.replace(\" \", \"_\")\n",
    "                    term = term.replace(\"-\", \"\")\n",
    "                    results.append(term)\n",
    "                    mapping[term] = claim.find(new_term)\n",
    "                    tmp_muji.append(new_term)\n",
    "\n",
    "    # 5 is a hyperparameter\n",
    "    if len(results) > 5:\n",
    "        assert -1 not in mapping.values()\n",
    "        results = sorted(mapping, key=mapping.get)\n",
    "    elif len(results) < 1:\n",
    "        results = first_wiki_term\n",
    "\n",
    "    return set(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Get noun phrases from hanlp consituency parsing tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup [HanLP](https://github.com/hankcs/HanLP) predictor (1 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    }
   ],
   "source": [
    "predictor = (hanlp.pipeline().append(\n",
    "    hanlp.load(\"MSR_TOK_ELECTRA_BASE_CRF\"),\n",
    "    output_key=\"tok\",\n",
    ").append(\n",
    "    hanlp.load(\"CTB9_CON_ELECTRA_SMALL\"),\n",
    "    output_key=\"con\",\n",
    "    input_key=\"tok\",\n",
    "))\n",
    "# FINE_ELECTRA_SMALL_ZH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will skip this process which for creating parsing tree when demo on class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hanlp_file = f\"data/hanlp_con_results.pkl\"\n",
    "if Path(hanlp_file).exists():\n",
    "    with open(hanlp_file, \"rb\") as f:\n",
    "        hanlp_results = pickle.load(f)\n",
    "else:\n",
    "    hanlp_results = [get_nps_hanlp(predictor, d) for d in TRAIN_DATA]\n",
    "    with open(hanlp_file, \"wb\") as f:\n",
    "        pickle.dump(hanlp_results, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get pages via wiki online api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b5c020dcba491b97d09f82b583f1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1134), Label(value='0 / 1134'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wikipedia\n",
    "doc_path = f\"data/train_doc5.jsonl\"\n",
    "if Path(doc_path).exists():\n",
    "    with open(doc_path, \"r\", encoding=\"utf8\") as f:\n",
    "        predicted_results = pd.Series([\n",
    "            set(json.loads(line)[\"predicted_pages\"])\n",
    "            for line in f\n",
    "        ])\n",
    "else:\n",
    "    train_df = pd.DataFrame(TRAIN_DATA)\n",
    "    train_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
    "    predicted_results = train_df.parallel_apply(get_pred_pages, axis=1)\n",
    "    save_doc(TRAIN_DATA, predicted_results, mode=\"train\")\n",
    "predicted_results.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Calculate our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "calculate_precision(TRAIN_DATA, predicted_results)\n",
    "calculate_recall(TRAIN_DATA, predicted_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Repeat the same process on test set\n",
    "Create parsing tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hanlp_test_file = f\"data/hanlp_con_test_results.pkl\"\n",
    "if Path(hanlp_test_file).exists():\n",
    "    with open(hanlp_test_file, \"rb\") as f:\n",
    "        hanlp_results = pickle.load(f)\n",
    "else:\n",
    "    hanlp_results = [get_nps_hanlp(predictor, d) for d in TEST_DATA]\n",
    "    with open(hanlp_test_file, \"wb\") as f:\n",
    "        pickle.dump(hanlp_results, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get pages via wiki online api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_path = f\"data/test_doc5.jsonl\"\n",
    "if Path(test_doc_path).exists():\n",
    "    with open(test_doc_path, \"r\", encoding=\"utf8\") as f:\n",
    "        test_results = pd.Series(\n",
    "            [set(json.loads(line)[\"predicted_pages\"]) for line in f])\n",
    "else:\n",
    "    test_df = pd.DataFrame(TEST_DATA)\n",
    "    test_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
    "    test_results = test_df.parallel_apply(get_pred_pages, axis=1)\n",
    "    save_doc(TEST_DATA, test_results, mode=\"test\")\n",
    "print(test_results.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ol4zFkSbjgXF"
   },
   "source": [
    "notebook2\n",
    "## PART 2. Sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GlliDsgXjisj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# built-in libs\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "\n",
    "# third-party libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from transformers import (\n",
    "#     AutoModelForSequenceClassification,\n",
    "#     AutoTokenizer,\n",
    "#     get_scheduler,\n",
    "# )\n",
    "# local libs\n",
    "from typing import Dict, Union\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "# import torch\n",
    "# from transformers import get_scheduler\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "\n",
    "def load_json(file_path: Union[Path, str]) -> pd.DataFrame:\n",
    "    \"\"\"jsonl_to_df read jsonl file and return a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path (Union[Path, str]): The jsonl file path.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The jsonl file content.\n",
    "\n",
    "    Example:\n",
    "        >>> read_jsonl_file(\"data/train.jsonl\")\n",
    "               id            label  ... predicted_label                                      evidence_list\n",
    "        0    3984          refutes  ...         REFUTES  [城市規劃是城市建設及管理的依據 ， 位於城市管理之規劃 、 建設 、 運作三個階段之首 ，...\n",
    "        ..    ...              ...  ...             ...                                                ...\n",
    "        945  3042         supports  ...         REFUTES  [北歐人相傳每當雷雨交加時就是索爾乘坐馬車出來巡視 ， 因此稱呼索爾為 “ 雷神 ” 。, ...\n",
    "\n",
    "        [946 rows x 10 columns]\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf8\") as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    return [json.loads(json_str) for json_str in json_list]\n",
    "\n",
    "\n",
    "def jsonl_dir_to_df(dir_path: Union[Path, str]) -> pd.DataFrame:\n",
    "    \"\"\"jsonl_dir_to_df read jsonl dir and return a pandas DataFrame.\n",
    "\n",
    "    This function will read all jsonl files in the dir_path and concat them.\n",
    "\n",
    "    Args:\n",
    "        dir_path (Union[Path, str]): The jsonl dir path.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The jsonl dir content.\n",
    "\n",
    "    Example:\n",
    "        >>> read_jsonl_dir(\"data/extracted_dir/\")\n",
    "               id            label  ... predicted_label                                      evidence_list\n",
    "        0    3984          refutes  ...         REFUTES  [城市規劃是城市建設及管理的依據 ， 位於城市管理之規劃 、 建設 、 運作三個階段之首 ，...\n",
    "        ..    ...              ...  ...             ...                                                ...\n",
    "        945  3042         supports  ...         REFUTES  [北歐人相傳每當雷雨交加時就是索爾乘坐馬車出來巡視 ， 因此稱呼索爾為 “ 雷神 ” 。, ...\n",
    "\n",
    "        [946 rows x 10 columns]\n",
    "    \"\"\"\n",
    "    print(f\"Reading and concatenating jsonl files in {dir_path}\")\n",
    "    return pd.concat(\n",
    "        [pd.DataFrame(load_json(file)) for file in Path(dir_path).glob(\"*.jsonl\")]\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_evidence_to_wiki_pages_mapping(\n",
    "    wiki_pages: pd.DataFrame,\n",
    ") -> Dict[str, Dict[int, str]]:\n",
    "    \"\"\"generate_wiki_pages_dict generate a mapping from evidence to wiki pages by evidence id.\n",
    "\n",
    "    Args:\n",
    "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
    "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
    "            If cache is None, return the result directly.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    def make_dict(x):\n",
    "        result = {}\n",
    "        sentences = re.split(r\"\\n(?=[0-9])\", x)\n",
    "        for sent in sentences:\n",
    "            splitted = sent.split(\"\\t\")\n",
    "            if len(splitted) < 2:\n",
    "                # Avoid empty articles\n",
    "                return result\n",
    "            result[splitted[0]] = splitted[1]\n",
    "        return result\n",
    "\n",
    "    # copy wiki_pages\n",
    "    wiki_pages = wiki_pages.copy()\n",
    "\n",
    "    # generate parse mapping\n",
    "    print(\"Generate parse mapping\")\n",
    "    wiki_pages[\"evidence_map\"] = wiki_pages[\"lines\"].parallel_map(make_dict)\n",
    "    # generate id to evidence_map mapping\n",
    "    print(\"Transform to id to evidence_map mapping\")\n",
    "    mapping = dict(\n",
    "        zip(\n",
    "            wiki_pages[\"id\"].to_list(),\n",
    "            wiki_pages[\"evidence_map\"].to_list(),\n",
    "        )\n",
    "    )\n",
    "    # release memory\n",
    "    del wiki_pages\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def save_checkpoint(model, ckpt_dir: str, current_step: int, mark: str = \"\"):\n",
    "    if mark != \"\":\n",
    "        mark += \"_\"\n",
    "    torch.save(model.state_dict(), f\"{ckpt_dir}/{mark}model.{current_step}.pt\")\n",
    "\n",
    "\n",
    "def load_model(model, ckpt_name, ckpt_dir: str):\n",
    "    model.load_state_dict(torch.load(f\"{ckpt_dir}/{ckpt_name}\"))\n",
    "    return model\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "J3BBLE3_hlPi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "TRAIN_DATA = load_json(\"data/public_train.jsonl\")\n",
    "TEST_DATA = load_json(\"data/public_test.jsonl\")\n",
    "DOC_DATA = load_json(\"data/train_doc5.jsonl\")\n",
    "\n",
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "_y = [LABEL2ID[data[\"label\"]] for data in TRAIN_DATA]\n",
    "# GT means Ground Truth\n",
    "TRAIN_GT = DOC_DATA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload wiki database (1 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and concatenating jsonl files in data/wiki-pages\n",
      "Generate parse mapping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16257f73305848659209e978df01ca28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=118776), Label(value='0 / 118776')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform to id to evidence_map mapping\n"
     ]
    }
   ],
   "source": [
    "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)\n",
    "del wiki_pages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision for sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evidence_macro_precision(\n",
    "    instance: Dict,\n",
    "    top_rows: pd.DataFrame,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculate precision for sentence retrieval\n",
    "    This function is modified from fever-scorer.\n",
    "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
    "\n",
    "    Args:\n",
    "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
    "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
    "\n",
    "        IMPORTANT!!!\n",
    "        instance (dict) should have the key of `evidence`.\n",
    "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]:\n",
    "        [1]: relevant and retrieved (numerator of precision)\n",
    "        [2]: retrieved (denominator of precision)\n",
    "    \"\"\"\n",
    "    this_precision = 0.0\n",
    "    this_precision_hits = 0.0\n",
    "\n",
    "    # Return 0, 0 if label is not enough info since not enough info does not\n",
    "    # contain any evidence.\n",
    "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
    "        # e[2] is the page title, e[3] is the sentence index\n",
    "        all_evi = [[e[2], e[3]]\n",
    "                   for eg in instance[\"evidence\"]\n",
    "                   for e in eg\n",
    "                   if e[3] is not None]\n",
    "        claim = instance[\"claim\"]\n",
    "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
    "                                      claim][\"predicted_evidence\"].tolist()\n",
    "\n",
    "        for prediction in predicted_evidence:\n",
    "            if prediction in all_evi:\n",
    "                this_precision += 1.0\n",
    "            this_precision_hits += 1.0\n",
    "\n",
    "        return (this_precision /\n",
    "                this_precision_hits) if this_precision_hits > 0 else 1.0, 1.0\n",
    "\n",
    "    return 0.0, 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate recall for sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evidence_macro_recall(\n",
    "    instance: Dict,\n",
    "    top_rows: pd.DataFrame,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculate recall for sentence retrieval\n",
    "    This function is modified from fever-scorer.\n",
    "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
    "\n",
    "    Args:\n",
    "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
    "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
    "\n",
    "        IMPORTANT!!!\n",
    "        instance (dict) should have the key of `evidence`.\n",
    "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]:\n",
    "        [1]: relevant and retrieved (numerator of recall)\n",
    "        [2]: relevant (denominator of recall)\n",
    "    \"\"\"\n",
    "    # We only want to score F1/Precision/Recall of recalled evidence for NEI claims\n",
    "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
    "        # If there's no evidence to predict, return 1\n",
    "        if len(instance[\"evidence\"]) == 0 or all(\n",
    "            [len(eg) == 0 for eg in instance]):\n",
    "            return 1.0, 1.0\n",
    "\n",
    "        claim = instance[\"claim\"]\n",
    "\n",
    "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
    "                                      claim][\"predicted_evidence\"].tolist()\n",
    "\n",
    "        for evidence_group in instance[\"evidence\"]:\n",
    "            evidence = [[e[2], e[3]] for e in evidence_group]\n",
    "            if all([item in predicted_evidence for item in evidence]):\n",
    "                # We only want to score complete groups of evidence. Incomplete\n",
    "                # groups are worthless.\n",
    "                return 1.0, 1.0\n",
    "        return 0.0, 1.0\n",
    "    return 0.0, 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the scores of sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_retrieval(\n",
    "    probs: np.ndarray,\n",
    "    df_evidences: pd.DataFrame,\n",
    "    ground_truths: pd.DataFrame,\n",
    "    top_n: int = 5,\n",
    "    cal_scores: bool = True,\n",
    "    save_name: str = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Calculate the scores of sentence retrieval\n",
    "\n",
    "    Args:\n",
    "        probs (np.ndarray): probabilities of the candidate retrieved sentences\n",
    "        df_evidences (pd.DataFrame): the candiate evidence sentences paired with claims\n",
    "        ground_truths (pd.DataFrame): the loaded data of dev.jsonl or test.jsonl\n",
    "        top_n (int, optional): the number of the retrieved sentences. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: F1 score, precision, and recall\n",
    "    \"\"\"\n",
    "    df_evidences[\"prob\"] = probs\n",
    "    top_rows = (\n",
    "        df_evidences.groupby(\"claim\").apply(\n",
    "        lambda x: x.nlargest(top_n, \"prob\"))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if cal_scores:\n",
    "        macro_precision = 0\n",
    "        macro_precision_hits = 0\n",
    "        macro_recall = 0\n",
    "        macro_recall_hits = 0\n",
    "\n",
    "        for i, instance in enumerate(ground_truths):\n",
    "            macro_prec = evidence_macro_precision(instance, top_rows)\n",
    "            macro_precision += macro_prec[0]\n",
    "            macro_precision_hits += macro_prec[1]\n",
    "\n",
    "            macro_rec = evidence_macro_recall(instance, top_rows)\n",
    "            macro_recall += macro_rec[0]\n",
    "            macro_recall_hits += macro_rec[1]\n",
    "\n",
    "        pr = (macro_precision /\n",
    "              macro_precision_hits) if macro_precision_hits > 0 else 1.0\n",
    "        rec = (macro_recall /\n",
    "               macro_recall_hits) if macro_recall_hits > 0 else 0.0\n",
    "        f1 = 2.0 * pr * rec / (pr + rec)\n",
    "\n",
    "    if save_name is not None:\n",
    "        # write doc7_sent5 file\n",
    "        with open(f\"data/{save_name}\", \"w\") as f:\n",
    "            for instance in ground_truths:\n",
    "                claim = instance[\"claim\"]\n",
    "                predicted_evidence = top_rows[\n",
    "                    top_rows[\"claim\"] == claim][\"predicted_evidence\"].tolist()\n",
    "                instance[\"predicted_evidence\"] = predicted_evidence\n",
    "                f.write(json.dumps(instance, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    if cal_scores:\n",
    "        return {\"F1 score\": f1, \"Precision\": pr, \"Recall\": rec}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference script to get probabilites for the candidate evidence sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function for sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gpvXpFwXszfv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pair_with_wiki_sentences(\n",
    "    mapping: Dict[str, Dict[int, str]],\n",
    "    df: pd.DataFrame,\n",
    "    negative_ratio: float,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Only for creating train sentences.\"\"\"\n",
    "    claims = []\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    idx = []\n",
    "    # positive\n",
    "    mappinglabel = {'supports':1, 'refutes':2}\n",
    "    for i in range(len(df)):\n",
    "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "        evidence_sets = df[\"evidence\"].iloc[i]\n",
    "        labelmap = mappinglabel[ df[\"label\"].iloc[i] ]\n",
    "        for evidence_set in evidence_sets:\n",
    "            sents = []\n",
    "            for evidence in evidence_set:\n",
    "                # evidence[2] is the page title\n",
    "                page = evidence[2].replace(\" \", \"_\")\n",
    "                # the only page with weird name\n",
    "                if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
    "                    continue\n",
    "                # evidence[3] is in form of int however, mapping requires str\n",
    "                sent_idx = str(evidence[3])\n",
    "                sents.append(mapping[page][sent_idx])\n",
    "\n",
    "            whole_evidence = \" \".join(sents)\n",
    "\n",
    "            claims.append(claim)\n",
    "            sentences.append(whole_evidence.replace(\" \",\"\"))\n",
    "            # idx.append(evidence[2])\n",
    "            labels.append(labelmap)\n",
    "\n",
    "    # negative\n",
    "    for i in range(len(df)):\n",
    "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "\n",
    "        evidence_set = set([(evidence[2], evidence[3])\n",
    "                            for evidences in df[\"evidence\"][i]\n",
    "                            for evidence in evidences])\n",
    "        predicted_pages = df[\"predicted_pages\"][i]\n",
    "        for page in predicted_pages:\n",
    "            page = page.replace(\" \", \"_\")\n",
    "            try:\n",
    "                page_sent_id_pairs = [\n",
    "                    (page, sent_idx) for sent_idx in mapping[page].keys()\n",
    "                ]\n",
    "            except KeyError:\n",
    "                # print(f\"{page} is not in our Wiki db.\")\n",
    "                continue\n",
    "\n",
    "            for pair in page_sent_id_pairs:\n",
    "                if pair in evidence_set:\n",
    "                    continue\n",
    "                text = mapping[page][pair[1]]\n",
    "                # `np.random.rand(1) <= 0.05`: Control not to add too many negative samples\n",
    "                if text != \"\" and np.random.rand(1) <= negative_ratio:\n",
    "                    claims.append(claim)\n",
    "                    sentences.append(text.replace(\" \",\"\"))\n",
    "                    labels.append(0)\n",
    "                    # idx.append(page)\n",
    "\n",
    "    # return pd.DataFrame({\"claim\": claims, \"text\": sentences, \"idx\": idx, \"label\": labels})\n",
    "    return pd.DataFrame({\"claim\": claims, \"text\": sentences,  \"label\": labels})\n",
    "\n",
    "\n",
    "def pair_with_wiki_sentences_eval(\n",
    "    mapping: Dict[str, Dict[int, str]],\n",
    "    df: pd.DataFrame,\n",
    "    is_testset: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Only for creating dev and test sentences.\"\"\"\n",
    "    claims = []\n",
    "    sentences = []\n",
    "    evidence = []\n",
    "    idx = []\n",
    "    predicted_evidence = []\n",
    "\n",
    "    # negative\n",
    "    for i in range(len(df)):\n",
    "        # if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "        #     continue\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "\n",
    "        predicted_pages = df[\"predicted_pages\"][i]\n",
    "        for page in predicted_pages:\n",
    "            page = page.replace(\" \", \"_\")\n",
    "            try:\n",
    "                page_sent_id_pairs = [(page, k) for k in mapping[page]]\n",
    "            except KeyError:\n",
    "                # print(f\"{page} is not in our Wiki db.\")\n",
    "                continue\n",
    "\n",
    "            for page_name, sentence_id in page_sent_id_pairs:\n",
    "                text = mapping[page][sentence_id]\n",
    "                if text != \"\":\n",
    "                    claims.append(claim)\n",
    "                    sentences.append(text.replace(\" \",\"\"))\n",
    "                    idx.append(page)\n",
    "                    if not is_testset:\n",
    "                        evidence.append(df[\"evidence\"].iloc[i])\n",
    "                    predicted_evidence.append([page_name, int(sentence_id)])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"claim\": claims,\n",
    "        \"text\": sentences,\n",
    "        # \"idx\": idx,\n",
    "        \"evidence\": evidence if not is_testset else None,\n",
    "        \"predicted_evidence\": predicted_evidence,\n",
    "    })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Setup training environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title  { display-mode: \"form\" }\n",
    "NEGATIVE_RATIO = 0.04  #@param {type:\"number\"}\n",
    "TOP_N = 5  #@param {type:\"integer\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Combine claims and evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4A5vWEzPiXGF",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using the following train data with 0 (Negative) and 1 (Positive)\n",
      "0    13735\n",
      "1     5159\n",
      "2     3308\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。</td>\n",
       "      <td>1787年由威廉·赫歇爾發現，並以威廉·莎士比亞的《仲夏夜之夢》中的妖精王后緹坦妮雅命名。</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>信天翁科的活動範圍位於北冰洋以及南太平洋，牠的翼展可達到3.7米，是世界上現存的翼展最大的鳥類。</td>\n",
       "      <td>信天翁的活動範圍位於南冰洋以及北太平洋。</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>南京大學附屬中學，從中國江蘇省遷移。</td>\n",
       "      <td>南京大學附屬中學，位於中國江蘇省南京市。</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>毒魚豆的萃取物被西印度群島的原住民發掘可以導致魚麻醉安靜，讓他們得以趁機徒手抓魚。</td>\n",
       "      <td>西印度群島的原住民發現這種植物的提取物可以令魚麻醉安靜，讓他們可以徒手抓魚。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>軟件開發是一項包括需求獲取、開發規劃、需求分析和設計、編程實現、軟件測試、版本控制的系統工程...</td>\n",
       "      <td>軟件開發是一項包括需求獲取、開發規劃、需求分析和設計、編程實現、軟件測試、版本控制的系統工程...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>國立臺灣大學應用力學研究所從1984年開始招收碩、博士班研究生，首任所長爲理論及應用力學專家...</td>\n",
       "      <td>於1984年招收第一屆碩、博士班研究生，首任所長爲理論及應用力學專家，美國國家工程院院士、中...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>威廉·倫琴拒絕定名新電子波爲倫琴射線，堅持稱作X射線。</td>\n",
       "      <td>有人提議將他發現的新射線定名爲“倫琴射線”，倫琴卻堅持用“X射線”這一名稱，產生X射線的機器...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>數學的基礎分支之一的幾何學在中古世紀的西方並未出現在教育中。</td>\n",
       "      <td>許多文化中都有幾何學的發展，包括許多有關長度、面積及體積的知識，在西元前六世紀泰勒斯的時代，...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>收入豐厚的湯姆克魯斯獲得了許多獎項。</td>\n",
       "      <td>作爲世界上收入最多的演員之一，他獲得了多項榮譽，包含三次金球獎和榮譽金棕櫚獎，以及三次奧斯卡...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>收入豐厚的湯姆克魯斯獲得了許多獎項。</td>\n",
       "      <td>他的電影在北美擁有超過45億票房，在全球擁有超過110億美元票房，使他成爲有史以來票房最高的巨星。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0                   天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。   \n",
       "1   信天翁科的活動範圍位於北冰洋以及南太平洋，牠的翼展可達到3.7米，是世界上現存的翼展最大的鳥類。   \n",
       "2                                 南京大學附屬中學，從中國江蘇省遷移。   \n",
       "3          毒魚豆的萃取物被西印度群島的原住民發掘可以導致魚麻醉安靜，讓他們得以趁機徒手抓魚。   \n",
       "4  軟件開發是一項包括需求獲取、開發規劃、需求分析和設計、編程實現、軟件測試、版本控制的系統工程...   \n",
       "5  國立臺灣大學應用力學研究所從1984年開始招收碩、博士班研究生，首任所長爲理論及應用力學專家...   \n",
       "6                        威廉·倫琴拒絕定名新電子波爲倫琴射線，堅持稱作X射線。   \n",
       "7                     數學的基礎分支之一的幾何學在中古世紀的西方並未出現在教育中。   \n",
       "8                                 收入豐厚的湯姆克魯斯獲得了許多獎項。   \n",
       "9                                 收入豐厚的湯姆克魯斯獲得了許多獎項。   \n",
       "\n",
       "                                                text  label  \n",
       "0      1787年由威廉·赫歇爾發現，並以威廉·莎士比亞的《仲夏夜之夢》中的妖精王后緹坦妮雅命名。      2  \n",
       "1                               信天翁的活動範圍位於南冰洋以及北太平洋。      2  \n",
       "2                               南京大學附屬中學，位於中國江蘇省南京市。      2  \n",
       "3             西印度群島的原住民發現這種植物的提取物可以令魚麻醉安靜，讓他們可以徒手抓魚。      1  \n",
       "4  軟件開發是一項包括需求獲取、開發規劃、需求分析和設計、編程實現、軟件測試、版本控制的系統工程...      1  \n",
       "5  於1984年招收第一屆碩、博士班研究生，首任所長爲理論及應用力學專家，美國國家工程院院士、中...      1  \n",
       "6  有人提議將他發現的新射線定名爲“倫琴射線”，倫琴卻堅持用“X射線”這一名稱，產生X射線的機器...      1  \n",
       "7  許多文化中都有幾何學的發展，包括許多有關長度、面積及體積的知識，在西元前六世紀泰勒斯的時代，...      2  \n",
       "8  作爲世界上收入最多的演員之一，他獲得了多項榮譽，包含三次金球獎和榮譽金棕櫚獎，以及三次奧斯卡...      1  \n",
       "9  他的電影在北美擁有超過45億票房，在全球擁有超過110億美元票房，使他成爲有史以來票房最高的巨星。      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pair_with_wiki_sentences(\n",
    "    mapping,\n",
    "    pd.DataFrame(TRAIN_GT),\n",
    "    NEGATIVE_RATIO,\n",
    ")\n",
    "counts = train_df[\"label\"].value_counts()\n",
    "print(\"Now using the following train data with 0 (Negative) and 1 (Positive)\")\n",
    "print(counts)\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8467</th>\n",
       "      <td>天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。</td>\n",
       "      <td>夢也有可能發生在其他睡眠階段中，不過這時的夢並不真切也難以記憶。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8468</th>\n",
       "      <td>天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。</td>\n",
       "      <td>這種「記憶抹除」的情況通常發生在一個人是自然緩和地從快速動眼睡眠階段經過慢波睡眠期而進入清醒狀態。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8469</th>\n",
       "      <td>天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。</td>\n",
       "      <td>西格蒙德·弗洛伊德創立了精神分析學，在1900年代早期的許多著作中闡述了夢的理論和解釋。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8470</th>\n",
       "      <td>信天翁科的活動範圍位於北冰洋以及南太平洋，牠的翼展可達到3.7米，是世界上現存的翼展最大的鳥類。</td>\n",
       "      <td>北冰洋有巴倫支海、波弗特海、楚克奇海、東西伯利亞海、格陵蘭海、哈得遜灣、哈得遜海峽、喀拉海、...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8471</th>\n",
       "      <td>信天翁科的活動範圍位於北冰洋以及南太平洋，牠的翼展可達到3.7米，是世界上現存的翼展最大的鳥類。</td>\n",
       "      <td>諸如“世界宗教”、“世界語言”、“世界政府”、“世界大戰”、“世界人口”、“世界經濟”或及“...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22197</th>\n",
       "      <td>朱邦復於1976年創制了由蔣經國命名的倉頡輸入法。</td>\n",
       "      <td>曾任中華民國總統、行政院院長、中國國民黨主席、制憲國民大會代表、國防部部長、行政院國軍退除役...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22198</th>\n",
       "      <td>加拿大國家銀行是加拿大第六大商業銀行，是具有信用創作功能的金融機構。</td>\n",
       "      <td>15世紀末，英國和法國殖民者開始探索北美洲的東岸，並在此建立殖民地。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22199</th>\n",
       "      <td>法國南部的摩納哥除了南部海岸線是靠地中海，其他三面皆被法國包圍。</td>\n",
       "      <td>這一帶對奧克西塔尼亞很大程度上說，哪些是奧克語(langued'oc）與法國南部的奧依語（l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22200</th>\n",
       "      <td>法國南部的摩納哥除了南部海岸線是靠地中海，其他三面皆被法國包圍。</td>\n",
       "      <td>地中海沿岸夏季炎熱乾燥，冬季溫暖溼潤，被稱作地中海性氣候。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22201</th>\n",
       "      <td>《啓示錄》有被收錄在新約聖經中。</td>\n",
       "      <td>《新約聖經》（ἩΚαινὴΔιαθήκη；NewTestament）是基督教所認爲的“《聖經...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13735 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  claim  \\\n",
       "8467                   天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。   \n",
       "8468                   天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。   \n",
       "8469                   天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。   \n",
       "8470   信天翁科的活動範圍位於北冰洋以及南太平洋，牠的翼展可達到3.7米，是世界上現存的翼展最大的鳥類。   \n",
       "8471   信天翁科的活動範圍位於北冰洋以及南太平洋，牠的翼展可達到3.7米，是世界上現存的翼展最大的鳥類。   \n",
       "...                                                 ...   \n",
       "22197                         朱邦復於1976年創制了由蔣經國命名的倉頡輸入法。   \n",
       "22198                加拿大國家銀行是加拿大第六大商業銀行，是具有信用創作功能的金融機構。   \n",
       "22199                  法國南部的摩納哥除了南部海岸線是靠地中海，其他三面皆被法國包圍。   \n",
       "22200                  法國南部的摩納哥除了南部海岸線是靠地中海，其他三面皆被法國包圍。   \n",
       "22201                                  《啓示錄》有被收錄在新約聖經中。   \n",
       "\n",
       "                                                    text  label  \n",
       "8467                    夢也有可能發生在其他睡眠階段中，不過這時的夢並不真切也難以記憶。      0  \n",
       "8468   這種「記憶抹除」的情況通常發生在一個人是自然緩和地從快速動眼睡眠階段經過慢波睡眠期而進入清醒狀態。      0  \n",
       "8469        西格蒙德·弗洛伊德創立了精神分析學，在1900年代早期的許多著作中闡述了夢的理論和解釋。      0  \n",
       "8470   北冰洋有巴倫支海、波弗特海、楚克奇海、東西伯利亞海、格陵蘭海、哈得遜灣、哈得遜海峽、喀拉海、...      0  \n",
       "8471   諸如“世界宗教”、“世界語言”、“世界政府”、“世界大戰”、“世界人口”、“世界經濟”或及“...      0  \n",
       "...                                                  ...    ...  \n",
       "22197  曾任中華民國總統、行政院院長、中國國民黨主席、制憲國民大會代表、國防部部長、行政院國軍退除役...      0  \n",
       "22198                 15世紀末，英國和法國殖民者開始探索北美洲的東岸，並在此建立殖民地。      0  \n",
       "22199  這一帶對奧克西塔尼亞很大程度上說，哪些是奧克語(langued'oc）與法國南部的奧依語（l...      0  \n",
       "22200                      地中海沿岸夏季炎熱乾燥，冬季溫暖溼潤，被稱作地中海性氣候。      0  \n",
       "22201  《新約聖經》（ἩΚαινὴΔιαθήκη；NewTestament）是基督教所認爲的“《聖經...      0  \n",
       "\n",
       "[13735 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"label\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('軟件開發是一項包括需求獲取、開發規劃、需求分析和設計、編程實現、軟件測試、版本控制的系統工程，其中包含任何最終獲得軟件產品的活動。',\n",
       " '曾擔任中共中央委員的羅榮桓，被授予過中華人民共和國元帥軍銜。')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['claim'].iloc[4],train_df['claim'].iloc[1650],"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Start training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l48WifjeIGui",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n",
      "AutoMM starts to create your model. ✨\n",
      "\n",
      "- Model will be saved to \"/workspace/AI_Text/tmp/a90295f8b21041368c3dedde440f789a-automm_sst\".\n",
      "\n",
      "- Validation metric is \"accuracy\".\n",
      "\n",
      "- To track the learning progress, you can open a terminal and launch Tensorboard:\n",
      "    ```shell\n",
      "    # Assume you have installed tensorboard\n",
      "    tensorboard --logdir /workspace/AI_Text/tmp/a90295f8b21041368c3dedde440f789a-automm_sst\n",
      "    ```\n",
      "\n",
      "Enjoy your coffee, and let AutoMM do the job ☕☕☕ Learn more at https://auto.gluon.ai\n",
      "\n",
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                         | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | model             | HFAutoModelForTextPrediction | 102 M \n",
      "1 | validation_metric | Accuracy                     | 0     \n",
      "2 | loss_func         | CrossEntropyLoss             | 0     \n",
      "-------------------------------------------------------------------\n",
      "102 M     Trainable params\n",
      "0         Non-trainable params\n",
      "102 M     Total params\n",
      "204.540   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c814606dc81a4c7e940b44b4249a236d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "import uuid\n",
    "from ray import tune\n",
    "from fastai.metrics import Recall\n",
    "# Recall(average='weighted')\n",
    "model_path = f\"./tmp/{uuid.uuid4().hex}-automm_sst\"\n",
    "predictor = MultiModalPredictor(label='label', path=model_path, presets=\"best_quality\")\n",
    "predictor.fit(train_df, \n",
    "        hyperparameters={\n",
    "            'model.hf_text.checkpoint_name':'hfl/chinese-lert-base',\n",
    "             \"optimization.max_epochs\": 20, \n",
    "             \"optimization.top_k\": 5\n",
    "        }\n",
    ")\n",
    "# hfl/chinese-bert-wwm-ext\n",
    "# hfl/chinese-macbert-base\n",
    "# hfl/chinese-roberta-wwm-ext-large\n",
    "# tune.choice(\n",
    "#    [\n",
    "#        'hfl/chinese-lert-base', 'hfl/chinese-bert-wwm-ext', 'hfl/chinese-macbert-base'\n",
    "#    ]\n",
    "# ),\n",
    "# presets=\"best_quality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.save(\"step2_split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。</td>\n",
       "      <td>1787年由威廉·赫歇爾發現，並以威廉·莎士比亞的《仲夏夜之夢》中的妖精王后緹坦妮雅命名。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>信天翁科的活動範圍位於北冰洋以及南太平洋，牠的翼展可達到3.7米，是世界上現存的翼展最大的鳥類。</td>\n",
       "      <td>信天翁的活動範圍位於南冰洋以及北太平洋。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>南京大學附屬中學，從中國江蘇省遷移。</td>\n",
       "      <td>南京大學附屬中學，位於中國江蘇省南京市。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>毒魚豆的萃取物被西印度群島的原住民發掘可以導致魚麻醉安靜，讓他們得以趁機徒手抓魚。</td>\n",
       "      <td>西印度群島的原住民發現這種植物的提取物可以令魚麻醉安靜，讓他們可以徒手抓魚。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>軟件開發是一項包括需求獲取、開發規劃、需求分析和設計、編程實現、軟件測試、版本控制的系統工程，其中包含任何最終獲得軟件產品的活動。</td>\n",
       "      <td>軟件開發是一項包括需求獲取、開發規劃、需求分析和設計、編程實現、軟件測試、版本控制的系統工程。換句話說，軟件開發就是一系列最終構建出軟件產品的活動。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               claim  \\\n",
       "0                                   天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。   \n",
       "1                   信天翁科的活動範圍位於北冰洋以及南太平洋，牠的翼展可達到3.7米，是世界上現存的翼展最大的鳥類。   \n",
       "2                                                 南京大學附屬中學，從中國江蘇省遷移。   \n",
       "3                          毒魚豆的萃取物被西印度群島的原住民發掘可以導致魚麻醉安靜，讓他們得以趁機徒手抓魚。   \n",
       "4  軟件開發是一項包括需求獲取、開發規劃、需求分析和設計、編程實現、軟件測試、版本控制的系統工程，其中包含任何最終獲得軟件產品的活動。   \n",
       "\n",
       "                                                                         text  \\\n",
       "0                               1787年由威廉·赫歇爾發現，並以威廉·莎士比亞的《仲夏夜之夢》中的妖精王后緹坦妮雅命名。   \n",
       "1                                                        信天翁的活動範圍位於南冰洋以及北太平洋。   \n",
       "2                                                        南京大學附屬中學，位於中國江蘇省南京市。   \n",
       "3                                      西印度群島的原住民發現這種植物的提取物可以令魚麻醉安靜，讓他們可以徒手抓魚。   \n",
       "4  軟件開發是一項包括需求獲取、開發規劃、需求分析和設計、編程實現、軟件測試、版本控制的系統工程。換句話說，軟件開發就是一系列最終構建出軟件產品的活動。   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your memory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>text</th>\n",
       "      <th>evidence</th>\n",
       "      <th>predicted_evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。</td>\n",
       "      <td>天王星（Uranus）是一顆在太陽系中離太陽第七近的青色行星，其體積在太陽系中排名第三、質量排名第四。</td>\n",
       "      <td>[[[4209, 4331, 天衛三, 2]]]</td>\n",
       "      <td>[天王星, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。</td>\n",
       "      <td>天王星的英文名稱Uranus來自古希臘神話的天空之神烏拉諾斯，是克洛諾斯的父親、宙斯的祖父。</td>\n",
       "      <td>[[[4209, 4331, 天衛三, 2]]]</td>\n",
       "      <td>[天王星, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。</td>\n",
       "      <td>在西方文化中，天王星是太陽系中唯一以希臘神祇命名的行星，其他行星都依照羅馬神祇命名。</td>\n",
       "      <td>[[[4209, 4331, 天衛三, 2]]]</td>\n",
       "      <td>[天王星, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。</td>\n",
       "      <td>與在古代就爲人們所知的五顆行星（水星、金星、火星、木星、土星）相比，天王星的亮度也是肉眼可見的，但由於較爲黯淡以及緩慢的繞行速度而未被古代的觀測者認定爲一顆行星。</td>\n",
       "      <td>[[[4209, 4331, 天衛三, 2]]]</td>\n",
       "      <td>[天王星, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。</td>\n",
       "      <td>直到1781年3月13日，威廉·赫歇耳爵士宣佈發現天王星，從而在太陽系的現代史上首度擴展已知的界限，也是第一顆使用望遠鏡發現的行星。</td>\n",
       "      <td>[[[4209, 4331, 天衛三, 2]]]</td>\n",
       "      <td>[天王星, 8]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              claim  \\\n",
       "0  天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。   \n",
       "1  天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。   \n",
       "2  天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。   \n",
       "3  天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。   \n",
       "4  天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。   \n",
       "\n",
       "                                                                                text  \\\n",
       "0                                天王星（Uranus）是一顆在太陽系中離太陽第七近的青色行星，其體積在太陽系中排名第三、質量排名第四。   \n",
       "1                                     天王星的英文名稱Uranus來自古希臘神話的天空之神烏拉諾斯，是克洛諾斯的父親、宙斯的祖父。   \n",
       "2                                         在西方文化中，天王星是太陽系中唯一以希臘神祇命名的行星，其他行星都依照羅馬神祇命名。   \n",
       "3  與在古代就爲人們所知的五顆行星（水星、金星、火星、木星、土星）相比，天王星的亮度也是肉眼可見的，但由於較爲黯淡以及緩慢的繞行速度而未被古代的觀測者認定爲一顆行星。   \n",
       "4                 直到1781年3月13日，威廉·赫歇耳爵士宣佈發現天王星，從而在太陽系的現代史上首度擴展已知的界限，也是第一顆使用望遠鏡發現的行星。   \n",
       "\n",
       "                   evidence predicted_evidence  \n",
       "0  [[[4209, 4331, 天衛三, 2]]]           [天王星, 0]  \n",
       "1  [[[4209, 4331, 天衛三, 2]]]           [天王星, 3]  \n",
       "2  [[[4209, 4331, 天衛三, 2]]]           [天王星, 4]  \n",
       "3  [[[4209, 4331, 天衛三, 2]]]           [天王星, 7]  \n",
       "4  [[[4209, 4331, 天衛三, 2]]]           [天王星, 8]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_evidences = pair_with_wiki_sentences_eval(\n",
    "    mapping=mapping,\n",
    "    df=pd.DataFrame(TRAIN_GT),\n",
    ")\n",
    "train_evidences.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation part (15 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predictor.save(\"step2_unbalance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c180b2457e84488985fdfd035bd8822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "# predictor = MultiModalPredictor.load(\"step2_unbalance\")\n",
    "probs = predictor.predict_proba(train_evidences)\n",
    "max_prob_indices = np.argmax(probs, axis=1)\n",
    "probs = probs.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[:, 1] += probs[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_values = [p[1] for p in probs]\n",
    "second_values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "train_evidences[\"probs\"] =second_values\n",
    "train_evidences = train_evidences[train_evidences[\"probs\"] > threshold ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_values = train_evidences[\"probs\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "train_results = evaluate_retrieval(\n",
    "    probs=second_values,\n",
    "    df_evidences=train_evidences,\n",
    "    ground_truths=TRAIN_GT,\n",
    "    top_n=5,\n",
    "    save_name=f\"train_doc5sent{TOP_N}.jsonl\",\n",
    ")\n",
    "print(f\"Training scores => {train_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fileName = f\"{train_results['F1 score']:.2f}-{train_results['Recall']:.2f}\"\n",
    "predictor.save(\"step2_base\"+fileName)\n",
    "f = open(\"step2.txt\", \"w\")\n",
    "f.write(str(train_results))\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Check on our test data\n",
    "(5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lVFusJqjmex-",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>text</th>\n",
       "      <th>evidence</th>\n",
       "      <th>predicted_evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>光學顯微鏡（Opticalmicroscope、Lightmicroscope）是一種利用光學透鏡產生影像放大效應的顯微鏡。</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>由物體入射的光被至少兩個光學系統（物鏡和目鏡）放大。</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>首先物鏡產生一個被放大實像，人眼通過作用相當於放大鏡的目鏡觀察這個已經被放大了的實像。</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>一般的光學顯微鏡有多個可以替換的物鏡，這樣觀察者可以按需要更換放大倍數，也就是增加放大倍率，放大倍率是由目鏡倍率乘上物鏡倍率所得來的。</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>這些物鏡一般被安置在一個可以轉動的物鏡盤上，轉動物鏡盤就可以使不同的物鏡方便地進入光路，物鏡盤的英文是Nosepiece，又譯作鼻輪。</td>\n",
       "      <td>None</td>\n",
       "      <td>[光學顯微鏡, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 claim  \\\n",
       "0  光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "1  光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "2  光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "3  光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "4  光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "\n",
       "                                                                  text  \\\n",
       "0       光學顯微鏡（Opticalmicroscope、Lightmicroscope）是一種利用光學透鏡產生影像放大效應的顯微鏡。   \n",
       "1                                           由物體入射的光被至少兩個光學系統（物鏡和目鏡）放大。   \n",
       "2                          首先物鏡產生一個被放大實像，人眼通過作用相當於放大鏡的目鏡觀察這個已經被放大了的實像。   \n",
       "3  一般的光學顯微鏡有多個可以替換的物鏡，這樣觀察者可以按需要更換放大倍數，也就是增加放大倍率，放大倍率是由目鏡倍率乘上物鏡倍率所得來的。   \n",
       "4  這些物鏡一般被安置在一個可以轉動的物鏡盤上，轉動物鏡盤就可以使不同的物鏡方便地進入光路，物鏡盤的英文是Nosepiece，又譯作鼻輪。   \n",
       "\n",
       "  evidence predicted_evidence  \n",
       "0     None         [光學顯微鏡, 0]  \n",
       "1     None         [光學顯微鏡, 3]  \n",
       "2     None         [光學顯微鏡, 4]  \n",
       "3     None         [光學顯微鏡, 5]  \n",
       "4     None         [光學顯微鏡, 6]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = load_json(\"data/test_doc5.jsonl\")\n",
    "\n",
    "test_evidences = pair_with_wiki_sentences_eval(\n",
    "    mapping,\n",
    "    pd.DataFrame(test_data),\n",
    "    is_testset=True,\n",
    ")\n",
    "test_evidences.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting the test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5923bdcf64a04e7a97a57f58055b42ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Start predicting the test data\")\n",
    "probs = predictor.predict_proba(test_evidences)\n",
    "probs = probs.to_numpy()\n",
    "second_values = [p[1] for p in probs]\n",
    "test_evidences[\"probs\"] =second_values\n",
    "test_evidences = test_evidences[test_evidences[\"probs\"] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_values = test_evidences[\"probs\"].values\n",
    "evaluate_retrieval(\n",
    "    probs=second_values,\n",
    "    df_evidences=test_evidences,\n",
    "    ground_truths=test_data,\n",
    "    top_n=TOP_N,\n",
    "    cal_scores=False,\n",
    "    save_name=f\"test_doc5sent{TOP_N}.jsonl\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lGzl8a5JteT7",
    "tags": []
   },
   "source": [
    "notebook3\n",
    "## PART 3. Claim verification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tgA1vcUyzjlx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    ")\n",
    "from utils import (\n",
    "    generate_evidence_to_wiki_pages_mapping,\n",
    "    jsonl_dir_to_df,\n",
    "    load_json,\n",
    "    load_model,\n",
    "    save_checkpoint,\n",
    ")\n",
    "\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "TRAIN_DATA = load_json(\"data/train_doc5sent5.jsonl\")\n",
    "TRAIN_PKL_FILE = Path(\"data/train_doc5sent5.pkl\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload wiki database (same as part 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and concatenating jsonl files in data/wiki-pages\n",
      "Generate parse mapping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292cc3b747da40838eb060a20b6bb24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=296938), Label(value='0 / 296938')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform to id to evidence_map mapping\n"
     ]
    }
   ],
   "source": [
    "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages,)\n",
    "del wiki_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_pages\n"
     ]
    }
   ],
   "source": [
    "print(\"wiki_pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def join_with_topk_evidence(\n",
    "    df: pd.DataFrame,\n",
    "    mapping: dict,\n",
    "    mode: str = \"train\",\n",
    "    topk: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"join_with_topk_evidence join the dataset with topk evidence.\n",
    "\n",
    "    Note:\n",
    "        After extraction, the dataset will be like this:\n",
    "               id     label         claim                           evidence            evidence_list\n",
    "        0    4604  supports       高行健...     [[[3393, 3552, 高行健, 0], [...  [高行健 （ ）江西赣州出...\n",
    "        ..    ...       ...            ...                                ...                     ...\n",
    "        945  2095  supports       美國總...  [[[1879, 2032, 吉米·卡特, 16], [...  [卸任后 ， 卡特積極參與...\n",
    "        停各种战争及人質危機的斡旋工作 ， 反对美国小布什政府攻打伊拉克...\n",
    "\n",
    "        [946 rows x 5 columns]\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset with evidence.\n",
    "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
    "        topk (int, optional): The topk evidence. Defaults to 5.\n",
    "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
    "            If cache is None, return the result directly.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with topk evidence_list.\n",
    "            The `evidence_list` column will be: List[str]\n",
    "    \"\"\"\n",
    "\n",
    "    # format evidence column to List[List[Tuple[str, str, str, str]]]\n",
    "    if \"evidence\" in df.columns:\n",
    "        df[\"evidence\"] = df[\"evidence\"].parallel_map(\n",
    "            lambda x: [[x]] if not isinstance(x[0], list) else [x]\n",
    "            if not isinstance(x[0][0], list) else x)\n",
    "\n",
    "    print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
    "    df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(lambda x: [\n",
    "            mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "            for evi_id, evi_idx in x  # for each evidence list\n",
    "        ][:topk] if isinstance(x, list) else [])\n",
    "    print(df[\"evidence_list\"][:5])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Setup training environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title  { display-mode: \"form\" }\n",
    "\n",
    "EVIDENCE_TOPK = 5  #@param {type:\"integer\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Concat claim and evidences\n",
    "join topk evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503a39c72fbf492fbabf269ab957b695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2837), Label(value='0 / 2837'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting evidence_list for the train mode ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde61855d25d4e4a9311005d6a20116e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2837), Label(value='0 / 2837'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [1787年由威廉 · 赫歇爾發現 ， 並以威廉 · 莎士比亞的 《 仲夏夜之夢 》 中的妖...\n",
      "1    [漂泊信天翁的翼展可達到 3.7 米 ， 是世界上現存的翼展最大的飛行鳥類 。, 信天翁的活...\n",
      "2    [組合F.I.R.的鍵盤手 。, 也是F.I.R.飛兒樂團的成員 ， 負責作曲 、 編曲 、...\n",
      "3    [香港國際機場全年24小時運作 ， 2010年處理 5,390 萬人次旅客及410萬公噸貨物...\n",
      "4    [黨委書記和校長列入中央管理的高校 ， 簡稱中管高校 ， 俗稱 “ 副部級高校 ” ， 爲中...\n",
      "Name: evidence_list, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>evidence</th>\n",
       "      <th>claim</th>\n",
       "      <th>predicted_pages</th>\n",
       "      <th>predicted_evidence</th>\n",
       "      <th>evidence_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2663</td>\n",
       "      <td>refutes</td>\n",
       "      <td>[[[4209, 4331, 天衛三, 2]]]</td>\n",
       "      <td>天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。</td>\n",
       "      <td>[天王星, 仲夏夜之夢_(消歧義), 仲夏夜_(羅文專輯), 緹坦妮雅, 天衛三, 磁層, ...</td>\n",
       "      <td>[[天衛三, 2], [天衛三, 3], [磁層, 1], [天衛三, 0], [緹坦妮雅,...</td>\n",
       "      <td>[1787年由威廉 · 赫歇爾發現 ， 並以威廉 · 莎士比亞的 《 仲夏夜之夢 》 中的妖...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2399</td>\n",
       "      <td>refutes</td>\n",
       "      <td>[[[2719, 2928, 信天翁科, 2]]]</td>\n",
       "      <td>信天翁科的活動範圍位於北冰洋以及南太平洋，牠的翼展可達到3.7米，是世界上現存的翼展最大的鳥類。</td>\n",
       "      <td>[牠, 翼展, 鳥, 信天翁科, 太平洋, 北冰洋, 世界, 南太平洋]</td>\n",
       "      <td>[[信天翁科, 4], [信天翁科, 2], [信天翁科, 1], [信天翁科, 0], [...</td>\n",
       "      <td>[漂泊信天翁的翼展可達到 3.7 米 ， 是世界上現存的翼展最大的飛行鳥類 。, 信天翁的活...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8075</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>[[[8075, null, null, null]]]</td>\n",
       "      <td>F.I.R.的團員有主唱Faye飛（詹雯婷）、吉他手Real阿沁（黃漢青）、鍵盤手Ian（陳...</td>\n",
       "      <td>[亞洲, 人, 陳建寧, 吉他, 吉他手, 飛_(消歧義), F._R._大衛, 詹雯婷, ...</td>\n",
       "      <td>[[陳建寧, 1], [阿沁, 1], [飛_(消歧義), 24], [飛_(消歧義), 1...</td>\n",
       "      <td>[組合F.I.R.的鍵盤手 。, 也是F.I.R.飛兒樂團的成員 ， 負責作曲 、 編曲 、...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8931</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>[[[8931, null, null, null]]]</td>\n",
       "      <td>香港國際機場全年24小時運作，它從2001年起一直躋身世界最佳機場，並8度獲評級爲全宇宙最佳機場。</td>\n",
       "      <td>[機場, 2001年, 24小時, 香港國際機場, 24_(電視劇), 世界, 宇宙]</td>\n",
       "      <td>[[香港國際機場, 7], [香港國際機場, 14], [香港國際機場, 15], [香港國...</td>\n",
       "      <td>[香港國際機場全年24小時運作 ， 2010年處理 5,390 萬人次旅客及410萬公噸貨物...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>332</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>[[[332, null, null, null]]]</td>\n",
       "      <td>北理工是歷史上最後一批副部級高校，黨委書記和校長列入中央管理的高校，簡稱中管高校，俗稱“副部...</td>\n",
       "      <td>[中華人民共和國, 黨委書記和校長列入中央管理的高校, 中央部屬高校, 高等學校, 歷史, 校長]</td>\n",
       "      <td>[[黨委書記和校長列入中央管理的高校, 0], [黨委書記和校長列入中央管理的高校, 1],...</td>\n",
       "      <td>[黨委書記和校長列入中央管理的高校 ， 簡稱中管高校 ， 俗稱 “ 副部級高校 ” ， 爲中...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id            label                      evidence  \\\n",
       "0  2663          refutes      [[[4209, 4331, 天衛三, 2]]]   \n",
       "1  2399          refutes     [[[2719, 2928, 信天翁科, 2]]]   \n",
       "2  8075  NOT ENOUGH INFO  [[[8075, null, null, null]]]   \n",
       "3  8931  NOT ENOUGH INFO  [[[8931, null, null, null]]]   \n",
       "4   332  NOT ENOUGH INFO   [[[332, null, null, null]]]   \n",
       "\n",
       "                                               claim  \\\n",
       "0                   天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。   \n",
       "1   信天翁科的活動範圍位於北冰洋以及南太平洋，牠的翼展可達到3.7米，是世界上現存的翼展最大的鳥類。   \n",
       "2  F.I.R.的團員有主唱Faye飛（詹雯婷）、吉他手Real阿沁（黃漢青）、鍵盤手Ian（陳...   \n",
       "3  香港國際機場全年24小時運作，它從2001年起一直躋身世界最佳機場，並8度獲評級爲全宇宙最佳機場。   \n",
       "4  北理工是歷史上最後一批副部級高校，黨委書記和校長列入中央管理的高校，簡稱中管高校，俗稱“副部...   \n",
       "\n",
       "                                     predicted_pages  \\\n",
       "0  [天王星, 仲夏夜之夢_(消歧義), 仲夏夜_(羅文專輯), 緹坦妮雅, 天衛三, 磁層, ...   \n",
       "1               [牠, 翼展, 鳥, 信天翁科, 太平洋, 北冰洋, 世界, 南太平洋]   \n",
       "2  [亞洲, 人, 陳建寧, 吉他, 吉他手, 飛_(消歧義), F._R._大衛, 詹雯婷, ...   \n",
       "3        [機場, 2001年, 24小時, 香港國際機場, 24_(電視劇), 世界, 宇宙]   \n",
       "4  [中華人民共和國, 黨委書記和校長列入中央管理的高校, 中央部屬高校, 高等學校, 歷史, 校長]   \n",
       "\n",
       "                                  predicted_evidence  \\\n",
       "0  [[天衛三, 2], [天衛三, 3], [磁層, 1], [天衛三, 0], [緹坦妮雅,...   \n",
       "1  [[信天翁科, 4], [信天翁科, 2], [信天翁科, 1], [信天翁科, 0], [...   \n",
       "2  [[陳建寧, 1], [阿沁, 1], [飛_(消歧義), 24], [飛_(消歧義), 1...   \n",
       "3  [[香港國際機場, 7], [香港國際機場, 14], [香港國際機場, 15], [香港國...   \n",
       "4  [[黨委書記和校長列入中央管理的高校, 0], [黨委書記和校長列入中央管理的高校, 1],...   \n",
       "\n",
       "                                       evidence_list  \n",
       "0  [1787年由威廉 · 赫歇爾發現 ， 並以威廉 · 莎士比亞的 《 仲夏夜之夢 》 中的妖...  \n",
       "1  [漂泊信天翁的翼展可達到 3.7 米 ， 是世界上現存的翼展最大的飛行鳥類 。, 信天翁的活...  \n",
       "2  [組合F.I.R.的鍵盤手 。, 也是F.I.R.飛兒樂團的成員 ， 負責作曲 、 編曲 、...  \n",
       "3  [香港國際機場全年24小時運作 ， 2010年處理 5,390 萬人次旅客及410萬公噸貨物...  \n",
       "4  [黨委書記和校長列入中央管理的高校 ， 簡稱中管高校 ， 俗稱 “ 副部級高校 ” ， 爲中...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(TRAIN_DATA),\n",
    "        mapping,\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "train_df.to_pickle(TRAIN_PKL_FILE, protocol=4)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prevent CUDA out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training (30 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_to_string(lst):\n",
    "    # 將 list 轉換為 string，並刪除所有空格\n",
    "    return \"[PAD]\".join(str(item).replace(' ', '') for item in lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = train_df[[\"label\",\"claim\",\"evidence_list\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>refutes</td>\n",
       "      <td>天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。</td>\n",
       "      <td>1787年由威廉·赫歇爾發現，並以威廉·莎士比亞的《仲夏夜之夢》中的妖精王后緹坦妮雅命名。[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>refutes</td>\n",
       "      <td>信天翁科的活動範圍位於北冰洋以及南太平洋，牠的翼展可達到3.7米，是世界上現存的翼展最大的鳥類。</td>\n",
       "      <td>漂泊信天翁的翼展可達到3.7米，是世界上現存的翼展最大的飛行鳥類。[PAD]信天翁的活動範圍...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>F.I.R.的團員有主唱Faye飛（詹雯婷）、吉他手Real阿沁（黃漢青）、鍵盤手Ian（陳...</td>\n",
       "      <td>組合F.I.R.的鍵盤手。[PAD]也是F.I.R.飛兒樂團的成員，負責作曲、編曲、製作與吉...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>香港國際機場全年24小時運作，它從2001年起一直躋身世界最佳機場，並8度獲評級爲全宇宙最佳機場。</td>\n",
       "      <td>香港國際機場全年24小時運作，2010年處理5,390萬人次旅客及410萬公噸貨物；2001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>北理工是歷史上最後一批副部級高校，黨委書記和校長列入中央管理的高校，簡稱中管高校，俗稱“副部...</td>\n",
       "      <td>黨委書記和校長列入中央管理的高校，簡稱中管高校，俗稱“副部級高校”，爲中華人民共和國中央部屬...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11341</th>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>伯明翰大學一共出了11位諾貝爾獎得主，其研究成果包括過敏性鼻炎的應用。</td>\n",
       "      <td>在學術研究方面，伯明翰大學的研究成果包括成功研製代替心臟運行的塑料心臟、維生素C的合成、利用...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11342</th>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>由其士集團負責承建並於2003年建築完成的翠豐臺位於香港新界，原是荃灣南豐紗廠的設廠地。</td>\n",
       "      <td>南豐紗廠(前稱：），位於香港荃灣柴灣角白田壩街45號，於1954年由人稱「棉紗大王」的南豐集...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11343</th>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>行經行政區域有中正區、大安區、信義區的臺北市仁愛路其中有100公尺寬的林園大道。</td>\n",
       "      <td>仁愛路亦爲臺北市著名的林蔭大道之一，路中央佈設公車專用道。[PAD]仁愛路爲臺北市的重要幹道...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11344</th>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>位處於臺南市由文化部所管轄的國定古蹟臺南北極殿，在臺南市區最高點，坐南朝北。</td>\n",
       "      <td>臺南北極殿，位於臺南市中西區，俗稱大上帝廟，爲中華民國國定古蹟[PAD]府城中和境鷲嶺北極殿...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11345</th>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>國立臺東大學有兩位副校長，第一任校長爲王家驥。</td>\n",
       "      <td>曾擔任臺灣省立臺東師範學校（今國立臺東大學）、臺灣省立高雄中學（今高雄市立高雄高級中學）等學...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11346 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label                                              claim  \\\n",
       "0              refutes                   天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。   \n",
       "1              refutes   信天翁科的活動範圍位於北冰洋以及南太平洋，牠的翼展可達到3.7米，是世界上現存的翼展最大的鳥類。   \n",
       "2      NOT ENOUGH INFO  F.I.R.的團員有主唱Faye飛（詹雯婷）、吉他手Real阿沁（黃漢青）、鍵盤手Ian（陳...   \n",
       "3      NOT ENOUGH INFO  香港國際機場全年24小時運作，它從2001年起一直躋身世界最佳機場，並8度獲評級爲全宇宙最佳機場。   \n",
       "4      NOT ENOUGH INFO  北理工是歷史上最後一批副部級高校，黨委書記和校長列入中央管理的高校，簡稱中管高校，俗稱“副部...   \n",
       "...                ...                                                ...   \n",
       "11341  NOT ENOUGH INFO                伯明翰大學一共出了11位諾貝爾獎得主，其研究成果包括過敏性鼻炎的應用。   \n",
       "11342  NOT ENOUGH INFO       由其士集團負責承建並於2003年建築完成的翠豐臺位於香港新界，原是荃灣南豐紗廠的設廠地。   \n",
       "11343  NOT ENOUGH INFO           行經行政區域有中正區、大安區、信義區的臺北市仁愛路其中有100公尺寬的林園大道。   \n",
       "11344  NOT ENOUGH INFO             位處於臺南市由文化部所管轄的國定古蹟臺南北極殿，在臺南市區最高點，坐南朝北。   \n",
       "11345  NOT ENOUGH INFO                            國立臺東大學有兩位副校長，第一任校長爲王家驥。   \n",
       "\n",
       "                                           evidence_list  \n",
       "0      1787年由威廉·赫歇爾發現，並以威廉·莎士比亞的《仲夏夜之夢》中的妖精王后緹坦妮雅命名。[...  \n",
       "1      漂泊信天翁的翼展可達到3.7米，是世界上現存的翼展最大的飛行鳥類。[PAD]信天翁的活動範圍...  \n",
       "2      組合F.I.R.的鍵盤手。[PAD]也是F.I.R.飛兒樂團的成員，負責作曲、編曲、製作與吉...  \n",
       "3      香港國際機場全年24小時運作，2010年處理5,390萬人次旅客及410萬公噸貨物；2001...  \n",
       "4      黨委書記和校長列入中央管理的高校，簡稱中管高校，俗稱“副部級高校”，爲中華人民共和國中央部屬...  \n",
       "...                                                  ...  \n",
       "11341  在學術研究方面，伯明翰大學的研究成果包括成功研製代替心臟運行的塑料心臟、維生素C的合成、利用...  \n",
       "11342  南豐紗廠(前稱：），位於香港荃灣柴灣角白田壩街45號，於1954年由人稱「棉紗大王」的南豐集...  \n",
       "11343  仁愛路亦爲臺北市著名的林蔭大道之一，路中央佈設公車專用道。[PAD]仁愛路爲臺北市的重要幹道...  \n",
       "11344  臺南北極殿，位於臺南市中西區，俗稱大上帝廟，爲中華民國國定古蹟[PAD]府城中和境鷲嶺北極殿...  \n",
       "11345  曾擔任臺灣省立臺東師範學校（今國立臺東大學）、臺灣省立高雄中學（今高雄市立高雄高級中學）等學...  \n",
       "\n",
       "[11346 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 將函數應用到整個列上\n",
    "train_df['evidence_list'] = train_df['evidence_list'].apply(list_to_string)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "supports           4819\n",
       "NOT ENOUGH INFO    3276\n",
       "refutes            3251\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6453</th>\n",
       "      <td>supports</td>\n",
       "      <td>鐒是在1961年被發現的。</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7557</th>\n",
       "      <td>supports</td>\n",
       "      <td>單一經濟共同體的貸款和買賣外匯是由央行負責。</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                   claim evidence_list\n",
       "6453  supports           鐒是在1961年被發現的。              \n",
       "7557  supports  單一經濟共同體的貸款和買賣外匯是由央行負責。              "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_rows = train_df[(train_df['evidence_list'] == '') & (train_df['label'] == 'supports')]\n",
    "selected_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n",
      "AutoMM starts to create your model. ✨\n",
      "\n",
      "- Model will be saved to \"/workspace/AI_Text/tmp/38aa6c0adf2742538b49d5bd353b0b58-automm_sst\".\n",
      "\n",
      "- Validation metric is \"acc\".\n",
      "\n",
      "- To track the learning progress, you can open a terminal and launch Tensorboard:\n",
      "    ```shell\n",
      "    # Assume you have installed tensorboard\n",
      "    tensorboard --logdir /workspace/AI_Text/tmp/38aa6c0adf2742538b49d5bd353b0b58-automm_sst\n",
      "    ```\n",
      "\n",
      "Enjoy your coffee, and let AutoMM do the job ☕☕☕ Learn more at https://auto.gluon.ai\n",
      "\n",
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                         | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | model             | HFAutoModelForTextPrediction | 102 M \n",
      "1 | validation_metric | Accuracy                     | 0     \n",
      "2 | loss_func         | CrossEntropyLoss             | 0     \n",
      "-------------------------------------------------------------------\n",
      "102 M     Trainable params\n",
      "0         Non-trainable params\n",
      "102 M     Total params\n",
      "204.540   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba9aa49d7bf4c3ba2e0e76ad76bd389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "import uuid\n",
    "model_path = f\"./tmp/{uuid.uuid4().hex}-automm_sst\"\n",
    "predictor = MultiModalPredictor(label='label', eval_metric='acc', path=model_path)\n",
    "predictor.fit(train_df, \n",
    "        hyperparameters={'model.hf_text.checkpoint_name': 'hfl/chinese-lert-base', \n",
    "                         'env.per_gpu_batch_size':4,\n",
    "                        'env.eval_batch_size_ratio':2,\n",
    "                         \"optimization.max_epochs\": 40, \"optimization.top_k\": 3}\n",
    ")\n",
    "predictor.save(\"step3_base\", standalone=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Make your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zLkfuoAE49mz",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting evidence_list for the eval mode ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a24d6a97f04c3992194a74734783e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=248), Label(value='0 / 248'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [顯微鏡泛指將微小不可見或難見物品之影像放大 ， 而能被肉眼或其他成像儀器觀察之工具 。, ...\n",
      "1    [蠶產絲 ， 蜜蜂產蜂蜜 ， 兩者都已被人類馴化 。, 家蠶 （ 學名 ： Bombyx m...\n",
      "2    [綠山城縣  ， 是波蘭的縣份 ， 位於該國西部 ， 由盧布斯卡省負責管轄 ， 首府設於綠山...\n",
      "3    [《 魂斷藍橋 》 （ Waterloo Bridge ） 是美國黑白電影 ， 由米高梅電影...\n",
      "4    [2015年以 《 刺客聶隱娘 》 獲得第68屆坎城影展最佳導演獎及第52屆金馬獎最佳導演獎...\n",
      "Name: evidence_list, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "# predictor = MultiModalPredictor.load(\"step3_base\")\n",
    "TEST_DATA = load_json(\"data/test_doc5sent5.jsonl\")\n",
    "TEST_PKL_FILE = Path(\"data/test_doc5sent5.pkl\")\n",
    "\n",
    "test_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(TEST_DATA),\n",
    "        mapping,\n",
    "        mode=\"eval\",\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "test_df.to_pickle(TEST_PKL_FILE, protocol=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>claim</th>\n",
       "      <th>predicted_pages</th>\n",
       "      <th>predicted_evidence</th>\n",
       "      <th>evidence_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5208</td>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>[光學顯微鏡, 電磁學, 顯微鏡, 肉眼, 物]</td>\n",
       "      <td>[[顯微鏡, 0], [光學顯微鏡, 0], [顯微鏡, 1], [顯微鏡, 6], [光學...</td>\n",
       "      <td>[顯微鏡泛指將微小不可見或難見物品之影像放大 ， 而能被肉眼或其他成像儀器觀察之工具 。, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1019</td>\n",
       "      <td>產絲的蠶或產蜜的蜜蜂爲提供間接經濟利益的昆蟲。</td>\n",
       "      <td>[昆蟲, 蠶]</td>\n",
       "      <td>[[昆蟲, 23], [蠶, 0], [蠶, 4], [昆蟲, 22], [蠶, 1]]</td>\n",
       "      <td>[蠶產絲 ， 蜜蜂產蜂蜜 ， 兩者都已被人類馴化 。, 家蠶 （ 學名 ： Bombyx m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8514</td>\n",
       "      <td>波蘭西部的綠山城縣平均每平方公里的土地有0人。</td>\n",
       "      <td>[土地, 人, 西部, 綠山城縣, 波蘭, 0]</td>\n",
       "      <td>[[綠山城縣, 0], [波蘭, 1], [土地, 10], [土地, 8], [土地, 4]]</td>\n",
       "      <td>[綠山城縣  ， 是波蘭的縣份 ， 位於該國西部 ， 由盧布斯卡省負責管轄 ， 首府設於綠山...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1874</td>\n",
       "      <td>VivienLeigh主演魂斷藍橋中的女配角。</td>\n",
       "      <td>[橋, 配角, 藍橋, 魂斷藍橋]</td>\n",
       "      <td>[[魂斷藍橋, 0], [藍橋, 2], [魂斷藍橋, 8], [魂斷藍橋, 1], [藍橋...</td>\n",
       "      <td>[《 魂斷藍橋 》 （ Waterloo Bridge ） 是美國黑白電影 ， 由米高梅電影...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8352</td>\n",
       "      <td>侯孝賢改編自唐代文言文學的電影獲得金馬獎最佳劇情片獎。</td>\n",
       "      <td>[侯孝賢, 劇情片, 金馬獎, 文學, 文言文, 電影]</td>\n",
       "      <td>[[侯孝賢, 2], [侯孝賢, 1], [侯孝賢, 0], [侯孝賢, 3], [金馬獎,...</td>\n",
       "      <td>[2015年以 《 刺客聶隱娘 》 獲得第68屆坎城影展最佳導演獎及第52屆金馬獎最佳導演獎...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>5668</td>\n",
       "      <td>中國的汾河是長江的支流之一。</td>\n",
       "      <td>[中國, 汾河, 長江]</td>\n",
       "      <td>[[汾河, 1], [汾河, 2], [汾河, 0], [汾河, 9], [汾河, 3]]</td>\n",
       "      <td>[汾河源於中國山西省北部忻州市寧武縣管涔山南側 ， 經太原盆地南流到新絳縣折向西 ， 由運城...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>4372</td>\n",
       "      <td>鐵達尼號首航的贊助公司在紐約。</td>\n",
       "      <td>[鐵達尼號_(1943年電影), 贊助, 紐約]</td>\n",
       "      <td>[[贊助, 2], [贊助, 4], [鐵達尼號_(1943年電影), 0], [紐約, 9...</td>\n",
       "      <td>[其中由於受到贊助 ， 使某些有意義的事項及心願 ， 由原來的不可能實現 ， 成爲可以實現的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>8250</td>\n",
       "      <td>麗臺科技力求『創新和好品質的信念』，成爲了亞洲知名的電腦及智慧醫療研發製造商。</td>\n",
       "      <td>[亞洲, 麗臺科技, 創新, 品質, 信念]</td>\n",
       "      <td>[[麗臺科技, 1], [麗臺科技, 0], [麗臺科技, 9], [麗臺科技, 4], [...</td>\n",
       "      <td>[麗臺科技是全球知名的電腦及智慧醫療研發製造商 、 NVIDIA長期合作伙伴 ， 以 「 研...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>7215</td>\n",
       "      <td>秋紅谷廣場在天災時有控制洪水蔓延速度的功能。</td>\n",
       "      <td>[功能, 秋紅谷廣場]</td>\n",
       "      <td>[[秋紅谷廣場, 0], [功能, 0]]</td>\n",
       "      <td>[秋紅谷生態公園 ， 又稱秋紅谷廣場 、 秋紅谷景觀生態公園 ， 通常直稱秋紅谷 ， 是位於...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>8911</td>\n",
       "      <td>澳洲醫學人物斯科特·布赫茲所屬的政黨屬於保守主義。</td>\n",
       "      <td>[海因裏希·赫茲, 政黨, 保守主義, 古斯塔夫·赫茲, 斯科特·布赫茲]</td>\n",
       "      <td>[[保守主義, 13], [斯科特·布赫茲, 0], [斯科特·布赫茲, 1], [保守主義...</td>\n",
       "      <td>[埃德蒙 · 伯克是18世紀反對法國大革命但支持美國革命的政治家 ， 被認爲是1790年代保...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>989 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                    claim  \\\n",
       "0    5208      光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "1    1019                  產絲的蠶或產蜜的蜜蜂爲提供間接經濟利益的昆蟲。   \n",
       "2    8514                  波蘭西部的綠山城縣平均每平方公里的土地有0人。   \n",
       "3    1874                  VivienLeigh主演魂斷藍橋中的女配角。   \n",
       "4    8352              侯孝賢改編自唐代文言文學的電影獲得金馬獎最佳劇情片獎。   \n",
       "..    ...                                      ...   \n",
       "984  5668                           中國的汾河是長江的支流之一。   \n",
       "985  4372                          鐵達尼號首航的贊助公司在紐約。   \n",
       "986  8250  麗臺科技力求『創新和好品質的信念』，成爲了亞洲知名的電腦及智慧醫療研發製造商。   \n",
       "987  7215                   秋紅谷廣場在天災時有控制洪水蔓延速度的功能。   \n",
       "988  8911                澳洲醫學人物斯科特·布赫茲所屬的政黨屬於保守主義。   \n",
       "\n",
       "                           predicted_pages  \\\n",
       "0                 [光學顯微鏡, 電磁學, 顯微鏡, 肉眼, 物]   \n",
       "1                                  [昆蟲, 蠶]   \n",
       "2                 [土地, 人, 西部, 綠山城縣, 波蘭, 0]   \n",
       "3                        [橋, 配角, 藍橋, 魂斷藍橋]   \n",
       "4             [侯孝賢, 劇情片, 金馬獎, 文學, 文言文, 電影]   \n",
       "..                                     ...   \n",
       "984                           [中國, 汾河, 長江]   \n",
       "985               [鐵達尼號_(1943年電影), 贊助, 紐約]   \n",
       "986                 [亞洲, 麗臺科技, 創新, 品質, 信念]   \n",
       "987                            [功能, 秋紅谷廣場]   \n",
       "988  [海因裏希·赫茲, 政黨, 保守主義, 古斯塔夫·赫茲, 斯科特·布赫茲]   \n",
       "\n",
       "                                    predicted_evidence  \\\n",
       "0    [[顯微鏡, 0], [光學顯微鏡, 0], [顯微鏡, 1], [顯微鏡, 6], [光學...   \n",
       "1         [[昆蟲, 23], [蠶, 0], [蠶, 4], [昆蟲, 22], [蠶, 1]]   \n",
       "2     [[綠山城縣, 0], [波蘭, 1], [土地, 10], [土地, 8], [土地, 4]]   \n",
       "3    [[魂斷藍橋, 0], [藍橋, 2], [魂斷藍橋, 8], [魂斷藍橋, 1], [藍橋...   \n",
       "4    [[侯孝賢, 2], [侯孝賢, 1], [侯孝賢, 0], [侯孝賢, 3], [金馬獎,...   \n",
       "..                                                 ...   \n",
       "984      [[汾河, 1], [汾河, 2], [汾河, 0], [汾河, 9], [汾河, 3]]   \n",
       "985  [[贊助, 2], [贊助, 4], [鐵達尼號_(1943年電影), 0], [紐約, 9...   \n",
       "986  [[麗臺科技, 1], [麗臺科技, 0], [麗臺科技, 9], [麗臺科技, 4], [...   \n",
       "987                              [[秋紅谷廣場, 0], [功能, 0]]   \n",
       "988  [[保守主義, 13], [斯科特·布赫茲, 0], [斯科特·布赫茲, 1], [保守主義...   \n",
       "\n",
       "                                         evidence_list  \n",
       "0    [顯微鏡泛指將微小不可見或難見物品之影像放大 ， 而能被肉眼或其他成像儀器觀察之工具 。, ...  \n",
       "1    [蠶產絲 ， 蜜蜂產蜂蜜 ， 兩者都已被人類馴化 。, 家蠶 （ 學名 ： Bombyx m...  \n",
       "2    [綠山城縣  ， 是波蘭的縣份 ， 位於該國西部 ， 由盧布斯卡省負責管轄 ， 首府設於綠山...  \n",
       "3    [《 魂斷藍橋 》 （ Waterloo Bridge ） 是美國黑白電影 ， 由米高梅電影...  \n",
       "4    [2015年以 《 刺客聶隱娘 》 獲得第68屆坎城影展最佳導演獎及第52屆金馬獎最佳導演獎...  \n",
       "..                                                 ...  \n",
       "984  [汾河源於中國山西省北部忻州市寧武縣管涔山南側 ， 經太原盆地南流到新絳縣折向西 ， 由運城...  \n",
       "985  [其中由於受到贊助 ， 使某些有意義的事項及心願 ， 由原來的不可能實現 ， 成爲可以實現的...  \n",
       "986  [麗臺科技是全球知名的電腦及智慧醫療研發製造商 、 NVIDIA長期合作伙伴 ， 以 「 研...  \n",
       "987  [秋紅谷生態公園 ， 又稱秋紅谷廣場 、 秋紅谷景觀生態公園 ， 通常直稱秋紅谷 ， 是位於...  \n",
       "988  [埃德蒙 · 伯克是18世紀反對法國大革命但支持美國革命的政治家 ， 被認爲是1790年代保...  \n",
       "\n",
       "[989 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_org = test_df.copy()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = test_df[[\"claim\",\"evidence_list\"]]\n",
    "test_df['evidence_list'] = test_df['evidence_list'].apply(list_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。</td>\n",
       "      <td>顯微鏡泛指將微小不可見或難見物品之影像放大，而能被肉眼或其他成像儀器觀察之工具。[PAD]光...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>產絲的蠶或產蜜的蜜蜂爲提供間接經濟利益的昆蟲。</td>\n",
       "      <td>蠶產絲，蜜蜂產蜂蜜，兩者都已被人類馴化。[PAD]家蠶（學名：Bombyxmori）是鱗翅目...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>波蘭西部的綠山城縣平均每平方公里的土地有0人。</td>\n",
       "      <td>綠山城縣，是波蘭的縣份，位於該國西部，由盧布斯卡省負責管轄，首府設於綠山城，面積1,571平...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VivienLeigh主演魂斷藍橋中的女配角。</td>\n",
       "      <td>《魂斷藍橋》（WaterlooBridge）是美國黑白電影，由米高梅電影公司於1940年出品...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>侯孝賢改編自唐代文言文學的電影獲得金馬獎最佳劇情片獎。</td>\n",
       "      <td>2015年以《刺客聶隱娘》獲得第68屆坎城影展最佳導演獎及第52屆金馬獎最佳導演獎與金馬獎最...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>中國的汾河是長江的支流之一。</td>\n",
       "      <td>汾河源於中國山西省北部忻州市寧武縣管涔山南側，經太原盆地南流到新絳縣折向西，由運城市的河津市...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>鐵達尼號首航的贊助公司在紐約。</td>\n",
       "      <td>其中由於受到贊助，使某些有意義的事項及心願，由原來的不可能實現，成爲可以實現的奇蹟，當中贊助...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>麗臺科技力求『創新和好品質的信念』，成爲了亞洲知名的電腦及智慧醫療研發製造商。</td>\n",
       "      <td>麗臺科技是全球知名的電腦及智慧醫療研發製造商、NVIDIA長期合作伙伴，以「研究創新、品質至...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>秋紅谷廣場在天災時有控制洪水蔓延速度的功能。</td>\n",
       "      <td>秋紅谷生態公園，又稱秋紅谷廣場、秋紅谷景觀生態公園，通常直稱秋紅谷，是位於臺灣台中市西屯區七...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>澳洲醫學人物斯科特·布赫茲所屬的政黨屬於保守主義。</td>\n",
       "      <td>埃德蒙·伯克是18世紀反對法國大革命但支持美國革命的政治家，被認爲是1790年代保守主義的主...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>989 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       claim  \\\n",
       "0        光學顯微鏡是以電磁學原理來將不可見或難見的微小物放大至肉眼可見的儀器。   \n",
       "1                    產絲的蠶或產蜜的蜜蜂爲提供間接經濟利益的昆蟲。   \n",
       "2                    波蘭西部的綠山城縣平均每平方公里的土地有0人。   \n",
       "3                    VivienLeigh主演魂斷藍橋中的女配角。   \n",
       "4                侯孝賢改編自唐代文言文學的電影獲得金馬獎最佳劇情片獎。   \n",
       "..                                       ...   \n",
       "984                           中國的汾河是長江的支流之一。   \n",
       "985                          鐵達尼號首航的贊助公司在紐約。   \n",
       "986  麗臺科技力求『創新和好品質的信念』，成爲了亞洲知名的電腦及智慧醫療研發製造商。   \n",
       "987                   秋紅谷廣場在天災時有控制洪水蔓延速度的功能。   \n",
       "988                澳洲醫學人物斯科特·布赫茲所屬的政黨屬於保守主義。   \n",
       "\n",
       "                                         evidence_list  \n",
       "0    顯微鏡泛指將微小不可見或難見物品之影像放大，而能被肉眼或其他成像儀器觀察之工具。[PAD]光...  \n",
       "1    蠶產絲，蜜蜂產蜂蜜，兩者都已被人類馴化。[PAD]家蠶（學名：Bombyxmori）是鱗翅目...  \n",
       "2    綠山城縣，是波蘭的縣份，位於該國西部，由盧布斯卡省負責管轄，首府設於綠山城，面積1,571平...  \n",
       "3    《魂斷藍橋》（WaterlooBridge）是美國黑白電影，由米高梅電影公司於1940年出品...  \n",
       "4    2015年以《刺客聶隱娘》獲得第68屆坎城影展最佳導演獎及第52屆金馬獎最佳導演獎與金馬獎最...  \n",
       "..                                                 ...  \n",
       "984  汾河源於中國山西省北部忻州市寧武縣管涔山南側，經太原盆地南流到新絳縣折向西，由運城市的河津市...  \n",
       "985  其中由於受到贊助，使某些有意義的事項及心願，由原來的不可能實現，成爲可以實現的奇蹟，當中贊助...  \n",
       "986  麗臺科技是全球知名的電腦及智慧醫療研發製造商、NVIDIA長期合作伙伴，以「研究創新、品質至...  \n",
       "987  秋紅谷生態公園，又稱秋紅谷廣場、秋紅谷景觀生態公園，通常直稱秋紅谷，是位於臺灣台中市西屯區七...  \n",
       "988  埃德蒙·伯克是18世紀反對法國大革命但支持美國革命的政治家，被認爲是1790年代保守主義的主...  \n",
       "\n",
       "[989 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load pretrained checkpoint: /workspace/AI_Text/step3_base/model.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871c4169398641bcb50813f76ae8ed33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictor = MultiModalPredictor.load(\"step3_base\")\n",
    "predicted_label = predictor.predict(test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_dataset = test_df_org\n",
    "predict_dataset[\"predicted_label\"] = predicted_label\n",
    "OUTPUT_FILENAME = \"submission.jsonl\"\n",
    "predict_dataset[[\"id\", \"predicted_label\", \"predicted_evidence\"]].to_json(\n",
    "    OUTPUT_FILENAME,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將預測 SUPPORTS 或是 REFUTES 但 predicted_evidence 是空的更改成 NOT ENOUGH INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "from utils import load_json\n",
    "\n",
    "data = load_json(OUTPUT_FILENAME)\n",
    "# data predicted_evidence is 0 change this label\n",
    "for item in data:\n",
    "    if item['predicted_label'] == 'NOT ENOUGH INFO':\n",
    "        continue\n",
    "    length = len(item['predicted_evidence'])\n",
    "    if length == 0:\n",
    "        print(item)\n",
    "        item['predicted_label'] = 'NOT ENOUGH INFO'\n",
    "# save this json \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_json(OUTPUT_FILENAME+\"_Del\", orient='records', lines=True, force_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "10286f3c74912972f7d1fdceceee5be5b7c77248e5efe5afcbc6a71f24d230fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
